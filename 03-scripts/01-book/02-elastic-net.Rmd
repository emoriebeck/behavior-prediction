---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Elastic Net  
Elastic Net Regression (ENR) proceeds from the observation that typical OLS-based regression minimizes bias but may have great variance. Using L1 (Ridge) and L2 (LASSO) approaches, which apply penalties to model estimates, ENR attempts to balance the trade-off between bias and variance by choosing the best penalties that minimize an information criterion or prediction error. Together, these both shrink coefficients and help with feature selection by forcing some of the coefficients to be zero. Because there are a large number of values the regularization parameter λ can take on, the typical solution is to use a cross-validation to test a number of possible $\lambda$ penalty values and choose the one with the one that matches a criterion like minimizing prediction error.  

In the present study, we used the `tidymodels` package in R to estimate the ENR models by calling the `logistic_reg()`, setting the engine as `“glmnet”`, and the mode as `“classification”`. The parameters tuned via rolling origin forecast validation were penalty and mixture, which were each set to 10 values. Next, we used the `select_best()` function with the method set to `“accuracy”` to allow the algorithm to automatically pick the best combination of penalty and mixture that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of penalty and mixture and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the `collect_metrics()` function.

## Functions  
```{r, eval = F}
dummy_vars <- c("o_value", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
                , "morning", "midday", "evening", "night", "argument"
                , "interacted", "lostSmthng", "late", "frgtSmthng", "brdSWk"
                , "excSWk", "AnxSWk", "tired", "sick", "sleeping", "class"
                , "music", "internet", "TV", "study")

time_vars <- c("sin2p", "sin1p", "cos2p", "cos1p")

c_fun <- function(m){
  # final model characteristics
  lambda <- min(m$fit$lambda)
  coefs <- stats::coef(m$fit, s = lambda)
  coefs <- coefs[, 1L, drop = TRUE]
  coefs <- coefs[setdiff(x = names(coefs), y = "(Intercept)")]
  return(coefs)
}

elnet_fun <- function(sid, outcome, group, set, time){
  # load the data
  load(sprintf("%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  d_train <- d_train %>% arrange(Full_Date) %>% select(-Full_Date) #%>%
    # mutate_at(vars(-o_value), ~as.numeric(as.character(.)))
  init <- ceiling(nrow(d_train)/3)
  
  d_train_cv <- rolling_origin(
    d_train, 
    initial = init,
    assess = 5,
    skip = 1,
    cumulative = TRUE
  )
  
  no_zv_fun <- function(x){
    if(is.numeric(x)) sd(x, na.rm = T) != 0 else length(table(x)) > 1
  }
  
  # set up the cross-valiation folds
  # set.seed(234)
  # d_train_cv <- vfold_cv(d_train, v = 10)
  
  # set up the data and formula
  mod_recipe <- recipe(
    o_value ~ .
    , data = d_train
    ) %>%
    step_zv(all_predictors(), -all_outcomes()) %>%
    step_nzv(all_predictors(), unique_cut = 35) %>%
    step_dummy(one_of(dummy_vars), -all_outcomes()) %>%
    step_normalize(-one_of(dummy_vars), -one_of(time_vars)) #%>%
    # estimate the means and standard deviations
    # prep(training = d_train, retain = TRUE)
  
  # set up the model specifications 
  tune_spec <- 
    logistic_reg(
      penalty = tune()
      , mixture = tune()
    ) %>% 
    set_engine("glmnet") %>% 
    set_mode("classification")
  
  # set up the ranges for the tuning functions 
  elnet_grid <- grid_regular(penalty()
                             , mixture()
                             , levels = 10)
  # set up the workflow: combine modeling spec with modeling recipe
  set.seed(345)
  elnet_wf <- workflow() %>%
    add_model(tune_spec) %>%
    add_recipe(mod_recipe)

  # combine the workflow, and grid to a final tuning model
  elnet_res <-
    elnet_wf %>%
    tune_grid(
      resamples = d_train_cv
      , grid = elnet_grid
      , control = control_resamples(save_pred = T)
      )
  save(elnet_res, file = sprintf("%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # load(sprintf("%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s.RData",
  #              res_path, sid, outcome, group, set))

  # plot the metrics across tuning parameters
  p <- elnet_res %>%
    collect_metrics() %>%
      ggplot(aes(penalty, mean, color = mixture)) +
      geom_point(size = 2) +
      facet_wrap(~ .metric, scales = "free", nrow = 2) +
      scale_x_log10(labels = scales::label_number()) +
      scale_color_gradient(low = "gray90", high = "red") +
      theme_classic()
  ggsave(p, file = sprintf("%s/05-results/01-glmnet/02-tuning-figures/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 8)
  
  # load(sprintf("%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData",
  #              res_path, sid, outcome, group, set, time))
  
  # select the best model based on AUC
  best_elnet <- elnet_res %>%
    # select_best("roc_auc")
    select_best("accuracy")
  
  # set up the workflow for the best model
  final_wf <- 
    elnet_wf %>% 
    finalize_workflow(best_elnet)
  
  # run the final best model on the training data and save
  final_elnet <- 
    final_wf %>%
    fit(data = d_train) 
  
    
  final_m <- final_elnet %>% 
    pull_workflow_fit() 
  final_coefs <- c_fun(final_m)
  
  best_elnet <- best_elnet %>%
    mutate(nvars = length(final_coefs[final_coefs != 0]))
  
  save(final_coefs, best_elnet,
       file = sprintf("%s/05-results/01-glmnet/07-final-model-param/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # load the test data
  load(sprintf("%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  # d_split$data$o_value <- factor(d_split$data$o_value)
  # d_split$data<-d_split$data %>% 
  #   mutate_at(vars(-Full_Date, -o_value), ~as.numeric(as.character(.)))
  
  # run the final fit workflow of the training and test data together
  final_fit <- 
    final_wf %>%
    last_fit(d_split) 
  save(final_elnet, final_fit
       , file = sprintf("%s/05-results/01-glmnet/03-final-training-models/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # final metrics (accuracy and roc)
  final_metrics <- final_fit %>%
    collect_metrics(summarize = T)
  save(final_metrics
       , file = sprintf("%s/05-results/01-glmnet/06-final-model-performance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # variable importance
  final_var_imp <- final_elnet %>% 
    pull_workflow_fit() %>% 
    vi() %>%
    slice_max(Importance, n = 10)
  save(final_var_imp
       , file = sprintf("%s/05-results/01-glmnet/05-variable-importance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # roc plot
  p_roc <- final_fit %>%
    collect_predictions() %>% 
    roc_curve(.pred_0, truth = o_value) %>% 
    autoplot() + 
    labs(title = sprintf("Participant %s: %s, %s, %s, %s"
                         , sid, outcome, group, set, time)) 
  ggsave(p_roc, file = sprintf("%s/05-results/01-glmnet/04-roc-curves/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 5)
  
  rm(list = c("final_var_imp", "final_metrics", "final_wf", "final_elnet", "final_fit"
              , "best_elnet", "elnet_res", "elnet_wf", "elnet_grid", "tune_spec", "mod_recipe"
              , "p", "p_roc", "d_split", "d_test", "d_train", "d_train_cv"))
  gc()
  return(T)
}
```

## Run Models  
```{r, eval = F}
plan(multisession(workers = 12L))
elnet_mods <- tibble(
  file = sprintf("%s/04-data/03-train-data", res_path) %>% list.files()
) %>%
  separate(file, c("SID", "outcome", "group", "set", "time"), sep = "_") %>%
  mutate(time = str_remove_all(time, ".RData")) %>%
  mutate(mod = 
           future_pmap(
           list(SID, outcome, group, set, time)
           , safely(elnet_fun, NA_real_)
           , .progress = T
           , .options = future_options(
             globals = c("res_path", "dummy_vars", "c_fun", "time_vars")
             , packages = c("plyr", "tidyverse", "glmnet", "tidymodels", "vip")
           )
           )
         ) 
closeAllConnections()
```
