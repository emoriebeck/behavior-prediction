--- 
title: "Idiographic prediction of loneliness and procrastination"
author: "Emorie D Beck"
institution: "Northwestern University Feinberg School of Medicine"
date: "`r Sys.Date()`"
site: 
  bookdown::bookdown_site:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    numbering: false
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "A longstanding goal of psychology is to predict the things people do, but tools to predict accurately future behaviors remain elusive. In the present study, we used intensive longitudinal data (N = 104; total assessments = 5,971) and three machine learning approaches to investigate the degree to which two behaviors – loneliness and procrastination – could be predicted from psychological (i.e. personality and affective states), situational (i.e. objective situations and psychological situation cues), and time (i.e. trends, diurnal cycles, time of day, and day of the week) phenomena from an idiographic, person-centered perspective. Rather than pitting persons against situations, such an approach allows psychological phenomena, situations, and time to jointly inform prediction. We find (1) a striking degree of accuracy across participants, (2) that a majority of participants models are best informed by both person and situation features, and (3) that the most important features vary greatly across people."
---


# Workspace  
## Packages  

```{r, echo = F}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, error = F)
options(knitr.kable.NA = '')
```


```{r packages}
library(knitr)              # creating tables
library(kableExtra)         # formatting and exporting tables
library(rio)                # importing html
library(readxl)             # read excel codebooks and documentation
library(psych)              # biscuit / biscwit
library(glmnet)             # elastic net regression
library(glmnetUtils)        # extension of basic elastic net with CV
library(caret)              # train and test for random forest
library(vip)                # variable importance
library(Amelia)             # multiple imputation (of time series)
library(lubridate)          # date wrangling
library(gtable)             # ggplot friendly tables
library(grid)               # ggplot friendly table rendering 
library(gridExtra)          # more helpful ggplot friendly table updates
library(plyr)               # data wranging
library(tidyverse)          # data wrangling
library(ggdist)             # distributional plots 
library(ggridges)           # more distributional plots 
library(cowplot)            # flexibly arrange multiple ggplot objects
library(tidymodels)         # tidy model workflow and selection
library(modeltime)          # tidy models for time series
library(furrr)              # mapping many models in parallel 
```


## Directory Path  
```{r path}
res_path <- "https://github.com/emoriebeck/behavior-prediction/raw/main"
local_path <- "~/Box/network/other projects/idio prediction"
```

## Codebook  
Each study has a separate codebook indexing matching, covariate, personality, and outcome variables. Moreover, these codebooks contain information about the original scale of the variable, any recoding of the variable (including binarizing outcomes, changing the scale, and removing missing data), reverse coding of scale variables, categories, etc.  
```{r codebook}
# list of all codebook sheets
ipcs_codebook <- import(file = sprintf("%s/01-codebooks/codebook.xlsx", res_path), which = 2) %>%
  as_tibble()
ipcs_codebook

outcomes <- ipcs_codebook %>% filter(category == "outcome") %>% select(trait, long_name)

ftrs <- import(file = sprintf("%s/01-codebooks/codebook.xlsx", res_path), which = 3) %>%
  as_tibble()
```

### Measures  
Participants responded to a large battery of trait and ESM measures as part of the larger study. The present study focuses on ESM measures whose use we preregistered. A full list of the collected measures for the study can be found in supplementary codebooks in the online materials on the OSF and GitHub. The measures collected at each wave were identical. ESM measures were used to estimate idiographic personality prediction models.  

#### ESM Measures  
##### Personality  
Personality was assessed using the full BFI-2 (Soto & John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 "disagree strongly" to 5 "agree strongly." Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants.  
```{r}
ipcs_codebook %>% filter(category == "BFI-2")
```


##### Affect  
Items capturing affect were initially pulled from the PANAS-X (Watson & Clark, 1994). In order to reduce redundancy, these were cross-referenced with the BFI-2 and duplicated items (e.g., "excited" were only asked once. Because we were not interested in scale score but in items, we further had research participants examine remaining items and asked them to indicate items that were not relevant to their experience. Finally, we added two "neutral" affect-related terms – goal-directed and purposeful. Each of these were rated on a 1 "disagree strongly" to 5 "agree strongly."  

```{r}
ipcs_codebook %>% filter(category == "Affect")
```

##### Binary Situations  
Binary situation indicators were derived by asking undergraduate research assistants to provide list of the common social, academic, and personal situations in which they tended to find themselves. From these, we derived a list of 19 unique situations. Separate items for arguing with or interacting with friends or relatives were composited in overall argument and interaction items. Participants checked a box for each event that occurred in the last hour (1 = occurred, 0 = did not occur).  

```{r}
ipcs_codebook %>% filter(category == "sit")
```

##### DIAMONDS Situation Features  
Psychological features of situations were measured using the ultra brief version of the "Situational Eight" DIAMONDS (Duty, Intellect, Adversity, Mating, pOsitivity, Negativity, Deception, and Sociality) scale (S8-I; Rauthmann  & Sherman, 2015). Items were measured on a 3-point scale from 1 "not at all" to 3 "totally."  

```{r}
ipcs_codebook %>% filter(category == "S8-I")
```

##### Timing Features  
The final set of features were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (Fisher & Soyster, 2019; . To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = \sin{\frac{2\pi}{12}}\ast\ {cumulative\ time}_t and 1 period sine = \sin{\frac{2\pi}{24}}\ast\ {cumulative\ time}_t).  

### Procedure  
Participants in this study were drawn from a larger personality study. All responded to two types of surveys: trait and state (Experience Sampling Method; ESM) measures, for which they were paid separately. Participants completed three waves of trait measures and two waves of state measures. For the first two waves, trait surveys were collected immediately before beginning the ESM protocol.  

#### Main Sample  
For the main sample, participants were recruited from the psychology subject pool at Washington University in St. Louis. Participants were told that the study posted on the recruitment website was the first wave of a longer longitudinal study they would be offered the opportunity to take part in.  

Participants were brought into the lab between October 2018 and December 2019, where a research assistant or the first author explained the study procedure to them and walked them through the consent procedure. If they consented, participants were led to a room where they could fill out a form to opt into the ESM portion of the study. They then completed baseline trait measures using the Qualtrics Survey Platform. After, the participants were debriefed, paid $10 in cash and, if they opted into the ESM portion of the study, the ESM survey procedure was explained to them.  

Participants then received ESM surveys four times per day for two weeks (target n = 56). The survey platform was built by the first author using the jsPsych library (De Leeuw, 2015). Additional JavaScript controllers were written for the purpose of this study and are available on the first author's GitHub. Start times were based on times that participants indicated they would like to receive their first survey based on their personal wake times. Surveys were sent every 4 hours, meaning that the surveys spanned a 12-hour period from the start time participants indicated. Participants received their first survey at their chosen time on the Monday following their in-lab session. They were compensated $.50 for each survey completed for a maximum of $28. To incentivize responding, participants who completed at least 50 surveys received a "bonus" for a total compensation of $30, which was distributed as an Amazon Gift Card.  

### Analytic Plan  

The present study tested three methods of machine learning classification models, some of which have been used for idiographic prediction in other studies (Fisher & Soyster, 2019; Kaiser & Butter, 2020): (1) Elastic Net Regression (Friedman, Hastie, & Tibshirani, 2010), (2) The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT; Elleman, McDougald, Condon, & Revelle, 2020), and (3) Random Forest Models (Kim et al., 2019).  

Because we have a large number of indicators to test, each of the methods used have variable selection features and, in some instances, other methods for reducing overfitting, as detailed below. To both reduce the number of indicators used in each test and to test which group of indicators are the most predictive of procrastination and loneliness, we will also test these in several sets: (1) Personality indicators (15), (2) Affective indicators (10), (3) Binary situation indicators (16), (4) DIAMONDS situation indicators (8), (5) Psychological indicators (personality + affect) (25), (6), Situation indicators (binary + DIAMONDS) (24), and (7) Full set (personality + affect + binary situations + DIAMONDS) (49). We will additionally test each of these with and without the 18 timing indicators, for a total set of 14 combinations of the 67 features.  

In each of these methods, we used cumulative rolling origin forecast validation,  which was comprised of the first 75% of the time series, and held out the remaining 25% of the data set for the test set. In the rolling origin forecast validation, we used the first one-third of the time series as the initial set, five observations as the validation set, and set skip to one, which roughly resulted in 10-15 rolling origin “folds.”  

Out of sample prediction was tested based on classification error and area under the ROC (receive operating characteristic) curve (AUC). Classification error is a simple estimate of the percentage of the test sample that was correctly classified by the model. In addition, the AUC will capture the trade-off between sensitivity and specificity across a threshold. In the present study, we used an AUC threshold of .5, which indicates binary classification at chance levels. ROC visualizations plot 1 - specificity (i.e. false positive rate: false positives / (false positives + true negatives)) against sensitivity (i.e. true positive rate: true positives / (true positives + false positives)).

## Demographics  

#### Trait  
```{r trait data combine, eval = F}
participants <- googlesheets4::sheets_read("https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit?usp=sharing", sheet = "ESM") %>%
  select(SID, Name, Email) %>%
  mutate(new = seq(1, n(), 1),
         new = ifelse(new < 10, paste("0", new, sep = ""), new))
1

old_names <- trait_codebook$`New #`

# wave 1 trait
baseline <- sprintf("%s/04-data/01-raw-data/baseline_05.07.20.csv", res_path) %>% 
  read_csv() %>%
  filter(!row_number() %in% c(1,2) & !is.na(SID) & SID %in% participants$SID) %>% 
  select(SID, StartDate, gender, YOB, race, ethnicity) %>%
  mutate(SID = mapvalues(SID, participants$SID, participants$new)) %>%
  mutate(wave = 1,
         gender = factor(gender, c(1,2), c("Male", "Female")),
         YOB = substr(YOB, nchar(YOB)-4+1, nchar(YOB)),
         race = mapvalues(race, 1:7, c(0,1,3,2,3,3,3)),
         ethnicity = ifelse(!is.na(ethnicity), 3, NA),
         race = ifelse(is.na(ethnicity), race, ifelse(ethnicity == 3, ethnicity)))  %>%
  select(-ethnicity)

save(baseline, 
     file = sprintf("%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData", res_path))
```

```{r}
load(url(sprintf("%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData", res_path)))
dem <- baseline %>%
  select(SID:race) %>%
  mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB),
         StartDate = as.Date(ymd_hms(StartDate)),
         race = factor(race, 0:3, c("White", "Black", "Asian", "Other"))) %>%
  select(-YOB) 

dem %>% 
  summarize(n = length(unique(SID)),
            gender = sprintf("%i (%.2f%%)",sum(gender == "Female"), sum(gender == "Female")/n()*100),
            age = sprintf("%.2f (%.2f)", mean(age, na.rm = T), sd(age, na.rm = T)),
            white = sprintf("%i (%.2f%%)"
                            , sum(race == "White", na.rm = T)
                            , sum(race == "White", na.rm = T)/n()*100),
            black = sprintf("%i (%.2f%%)"
                            , sum(race == "Black", na.rm = T)
                            , sum(race == "Black", na.rm = T)/n()*100),
            asian = sprintf("%i (%.2f%%)"
                            , sum(race == "Asian", na.rm = T)
                            , sum(race == "Asian", na.rm = T)/n()*100),
            other = sprintf("%i (%.2f%%)"
                            , sum(race == "Other", na.rm = T)
                            , sum(race == "Other", na.rm = T)/n()*100),
            StartDate = sprintf("%s (%s - %s)", median(StartDate), 
                                min(StartDate), max(StartDate)))

dem %>%
  kable(., "html"
        , col.names = c("ID", "Start Date", "Gender", "Race/Ethnicity", "Age")
        , align = rep("c", 5)
        , caption = "<strong>Table S1</strong><br><em>Descriptive Statistics of Participants at Baseline<em>") %>%
  kable_styling(full_width = F) %>%
    scroll_box(height = "900px")
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Data Cleaning {#cleaning}

## Pre-Share Cleaning  
For the purposes of this study, we are only sharing the data used in this study (for both items and participants). ESM data were pre-cleaned because data imported from jsPsych are in a format not easily interpretable within R. An R script that shows the full procedure for extracting jsPscyh data is available on the OSF page for this study. Trait data were downloaded from Qualtrics and directly imported and cleaned as shown below.  

In addition, in order to anonymize participants, we have changed their ID's using our master list, which we cannot make available because it contains identifying information.  

```{r ipcs esm data combine, eval = F}
participants <- googlesheets4::sheets_read("https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit#gid=16299281", sheet = "ESM") %>%
  select(SID, Name, Email) %>%
  mutate(new = seq(1, nrow(.), 1),
         new = ifelse(new < 10, paste("0", new, sep = ""), new))
1

# wave 1 esm 
load(sprintf("%s/04-data/01-raw-data/clean_data_w1_2020-06-08.RData", res_path))

# combine waves  
bfi <- BFI %>% distinct() %>%
  mutate(SID = mapvalues(SID, participants$SID, participants$new)) %>%
  rename(orig_itemname = item) %>%
  left_join(ipcs_codebook %>% select(category, trait, facet, itemname, orig_itemname, reverse_code))
old.names <- ipcs_codebook$orig_itemname

emo <- emotion %>%
  select(-trait, -facet) %>%
  mutate(item = str_remove_all(item, "E_")) %>%
  mutate(SID = mapvalues(SID, participants$SID, participants$new)) %>%
  rename(trait = item) %>%
  left_join(ipcs_codebook %>% select(category, trait, facet, itemname, reverse_code))

sit  <- sit %>%
  filter(item %in% old.names) %>%
  select(-trait, -facet, -answer) %>%
  mutate(SID = mapvalues(SID, participants$SID, participants$new)) %>%
  rename(orig_itemname = item) %>%
  left_join(ipcs_codebook %>% select(category, trait, facet, itemname, orig_itemname, reverse_code))

ds8 <- DS8 %>% 
  select(-trait, -facet, -answer) %>%
  mutate(SID = mapvalues(SID, participants$SID, participants$new)) %>%
  rename(orig_itemname = item) %>%
  left_join(ipcs_codebook %>% select(category, trait, facet, itemname, orig_itemname, reverse_code))

save(bfi, emo, sit, ds8, file = sprintf("%s/04-data/esm_cleaned_combined_2021-04-07.RData", res_path))
rm(list = ls())
```

```{r}
train_fun <- function(x) {
  if(length(unique(x[!is.na(x)]))==1){
    replace <- c(0,1)[!0:1 %in% unique(x[!is.na(x)])[1]]
    x[sample(1:length(x), 1)] <- replace
  } else if (any(table(x) == 1)){
    replace <- c(0,1)[which(table(x) <= 1)]
    x[sample(1:length(x), 1)] <- replace
  }
  x
}

test_fun <- function(x) {
  if(length(unique(x[!is.na(x)]))==1){
    replace <- c(0,1)[!0:1 %in% unique(x[!is.na(x)])[1]]
    x[sample(1:length(x), 1)] <- replace
  }
  x
}
```


Now, we'll load in the cleaned and de-identified data.  

```{r}
load(url(sprintf("%s/04-data/01-raw-data/esm_cleaned_combined_2021-04-07.RData", res_path)))
```

## ESM Data Setup  
Next, we need to make sure that all time information for IPCS is available. Specifically, this will allow us to control for overnight periods and unequal spacing between measurement occasions.  

### Timing  
First, we need to create empty rows in the data where there are missing assessments from the four target surveys per day as well as for the overnight periods. The function below uses the time stamp to figure out which blocks are missing and add those empty rows by indexing the time stamp of collected surveys as well as participants chosen start times.    

```{r ipcs esm data cleaning}
missing_fun <- function(d){
  first_day <- unique(d$StartDate) # get first day
  hourBlock <- unique(d$`Hour Block 1`) # get first hour block
  max_day <- max(d$Day); max_day <- ifelse(max_day < 14, 14, max_day) # get number of days
  d2 <- d %>% #mutate(StartDate = ifelse(is.na(StartDate), min(Date, na.), StartDate))
    full_join(crossing(
      Day = seq(0,max_day,1),
      HourBlock = 1:6,
      StartDate = first_day,
      `Hour Block 1` = hourBlock)) %>% # cross existing data with "perfect" data
    arrange(Day, HourBlock) %>%
    mutate(Date = StartDate + Day,
           Hour = ifelse(is.na(Hour), `Hour Block 1` + (HourBlock-1)*4, Hour),
           Minute = ifelse(is.na(Minute), "00", Minute))
  d2$Date[d2$Hour > 23] <- d2$Date[d2$Hour > 23] + 1 # some day blocks span days
  d2$Hour[d2$Hour > 23] <- d2$Hour[d2$Hour > 23] - 24 # some day blocks span days
  d2 <- d2 %>% mutate(
    Full_Date = sprintf("%s %s:%s", as.character(Date), Hour, Minute)) %>%
    select(-`Hour Block 1`, -StartDate) 
}

# create a data frame of timing info 
ipcs_times <- emo %>% 
  select(SID, StartDate, Date, Hour, Minute, Day, `Hour Block 1`, HourBlock) %>%
  distinct() %>%
  mutate(Minute = str_remove_all(Minute, ".csv"),
         Minute = ifelse(as.numeric(Minute) < 10, sprintf("0%s", Minute), Minute)) %>%
  arrange(SID, Date) %>%
  group_by(SID) %>%
  nest() %>%
  ungroup() %>%
  mutate(data = map(data, missing_fun)) %>%
  unnest(data) %>%
  arrange(SID, Date, Hour) %>%
  group_by(SID) %>%
  mutate(all_beeps = seq(1, n(), 1)) %>%
  ungroup()
```

Here's the result
```{r}
ipcs_times
```


### Personality  
Now it's time to wrangle the personality data. As a reminder, personality was assessed using the full BFI-2 (Soto & John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 "disagree strongly" to 5 "agree strongly." Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants.  

#### Wrangle Raw Data
```{r}
# join with codebook, reverse code, composite within facets and spread to wide format
bfi_wide <- bfi %>% 
  select(SID, Date, Hour, Minute, trait, facet, value = responses2, reverse_code) %>%
  mutate(Minute = str_remove_all(Minute, ".csv"),
         Minute = ifelse(as.numeric(Minute) < 10, sprintf("0%s", Minute), Minute),
         Full_Date = sprintf("%s %s:%s", as.character(Date), Hour, Minute)) %>%
  mutate(value = as.numeric(value),
         value = ifelse(!is.na(reverse_code) & reverse_code == "yes", reverse.code(-1, value, mini = 1, maxi = 5), value)) %>%
  select(-reverse_code) %>%
  group_by(SID, trait, facet, Full_Date) %>%
  summarize(value = mean(value, na.rm = T)) %>%
  ungroup() %>%
  pivot_wider(names_from = c("trait", "facet")
              , values_from = "value"
              , names_sep = "_") %>%
  group_by(SID) %>%
  arrange(SID, lubridate::ymd_hm(Full_Date)) %>%
  mutate(all_beeps = seq(1, n(), 1)) %>%
  ungroup()
```

#### Multiple Imputation  
These data were collected using a planned missing design, so we need to impute data for the planned missing components.  
```{r}
# run MI
bfi_mi <- data.frame(unclass(bfi_wide %>% select(-Full_Date)))
set.seed(5)
bfi_mi <- amelia(bfi_mi, m = 1, ts = "all_beeps", cs = "SID")$imputations[[1]] %>%
  as_tibble() %>%
  full_join(bfi_wide %>% select(SID, Full_Date, all_beeps)) %>%
  select(-all_beeps)
bfi_mi
```

Personality data:  
```{r}
bfi_mi
```

Now back to long format  

```{r}
bfi_long <- bfi_mi %>%
  pivot_longer(cols = c(-SID, -Full_Date)
               , names_to = c("trait", "facet")
               , values_to = "value"
               , names_sep = "_") %>%
  mutate(category = "BFI-2")
```

### Other Measured Features  
Emotion and Situation (Binary and DIAMONDS) Data (not planned missing, so no need to impute):  

```{r}
features <- emo %>%
  full_join(sit) %>%
  full_join(ds8) %>%
  select(SID, category, trait, facet, itemname, Date, Hour, Minute, Day, HourBlock, value = responses2) %>%
  mutate(Minute = str_remove_all(Minute, ".csv"),
         Minute = ifelse(as.numeric(Minute) < 10, sprintf("0%s", Minute), Minute),
         Full_Date = sprintf("%s %s:%s", as.character(Date), Hour, Minute),
         value = as.numeric(value)) %>%
  group_by(SID, category, trait, facet, Full_Date) %>% 
  summarize(value = max(value)) %>%
  ungroup()
```

```{r}
features
```

### Timing Features  
Finally, we'll create the timing features. These were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (e.g., Fisher & Soyster, 2019). To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = \sin{\frac{2\pi}{12}}\ast\ {cumulative\ time}_t and 1 period sine = \sin{\frac{2\pi}{24}}\ast\ {cumulative\ time}_t).  

```{r}
time_features <- ipcs_times %>%
  mutate(wkday = wday(Full_Date, label = T)
         , Mon =     ifelse(wkday == "Mon", 1, 0)
         , Tue =     ifelse(wkday == "Tue", 1, 0)
         , Wed =     ifelse(wkday == "Wed", 1, 0)
         , Thu =     ifelse(wkday == "Thu", 1, 0)
         , Fri =     ifelse(wkday == "Fri", 1, 0)
         , Sat =     ifelse(wkday == "Sat", 1, 0)
         , Sun =     ifelse(wkday == "Sun", 1, 0)
         , morning = ifelse(Hour  >= 5  & Hour < 11, 1, 0)
         , midday =  ifelse(Hour  >= 11 & Hour < 17, 1, 0)
         , evening = ifelse(Hour  >= 5  & Hour < 22, 1, 0)
         , night =   ifelse(Hour  >= 22 & Hour < 5,  1, 0)) %>%
  
  ## sequential time differences for each persn
  group_by(SID) %>%
  mutate(tdif =      as.numeric(difftime(ymd_hm(Full_Date), lag(ymd_hm(Full_Date)), units = "hours"))) %>%
  filter(is.na(tdif) | tdif > 1) %>%
  mutate(tdif =      as.numeric(difftime(ymd_hm(Full_Date), lag(ymd_hm(Full_Date)), units = "hours"))
         , tdif =    ifelse(is.na(tdif), 0, tdif)
         , cumsumT = cumsum(tdif)) %>%
  ungroup() %>%
  
  ## timing variables
  mutate(linear =    as.numeric(scale(cumsumT))
         , quad =    linear^2
         , cub =     linear^3
         , sin1p =   sin(((2*pi)/24)*cumsumT)
         , sin2p =   sin(((2*pi)/12)*cumsumT)
         , cos1p =   cos(((2*pi)/24)*cumsumT)
         , cos2p =   cos(((2*pi)/12)*cumsumT)
         ) %>%
  
  ## keep key variables and reshape
  select(SID, Full_Date, Mon:night, linear:cos2p) %>%
  pivot_longer(cols = c(-SID, -Full_Date)
               , names_to = "trait"
               , values_to = "value") %>%
  mutate(category = "time"
         , facet = trait)
```

```{r}
time_features
```

### Combine Features  
Now, let's bring the personality, affect/situation/DIAMONDS, and timing features together.  

```{r}
all_features <- bfi_long %>%
  full_join(features) %>%
  full_join(ipcs_times %>% select(SID, Full_Date)) %>%
  full_join(time_features) %>%
  arrange(SID, category, trait, facet, Full_Date)
all_features
```

## Setup for Idiographic Machine Learning Models  

The last step is the most important. We need to: 
Separate the data for each outcome, participant, and feature set combination. In addition, the outcomes need to be lagged such that same time point features will be predicting "future" behavior. Moreover, the data must be split into training (first 75\%) and test sets (last 25\%). As we do this, we will also remove participants who have no variance in the outcome in either training or test sets as we can't (statistically) predict things without variance (even if no variance suggests a good prediction!).  

The feature sets are as follows:  

- Psychological: Big Five (BFI-2)  
- Psychological: Affect  
- Psychological: Big Five + Affect  
- Situations: Binary  
- Situations: DIAMONDS  
- Situations: Binary + DIAMONDS
- Full: Big Five + Affect + Binary + DIAMONDS  

Each of these will be tested with and without the timing features for a total number of 14 feature sets.  

For now, I'm not going to run the chunk below because it takes a long time. All the resulting files can be found in the online materials:  

- 04-data/02-raw-data: data before being split into training and test  
- 04-data/03-train-data: training data for each participant x outcome x feature set combination (14)  
- 04-data/04-test-data: test data for each participant x outcome x feature set combination (14)  

```{r, eval = F}
save_fun <- function(d, group, set, outcome, SID, time){
  print(paste(SID, outcome, group, set, time))
    d_split <- initial_time_split(d, prop = 0.75)
    d_train <- training(d_split)
    d_test  <- testing(d_split)
  if(any(table(d_train$o_value) < 2) | sd(d_train$o_value) == 0) {
    return(NA) # no variance == can't use that participant
  } else {
    d_train <- d_train %>%
      mutate_at(vars(one_of(dummy_vars)), train_fun) %>%
      mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor)
    ret <- F # this is indexing if there were any other issues or concerns to be aware of
  }
    if(length(unique(d_test$o_value[!is.na(d_test$o_value)])) == 1){
    d_test <- d_test %>%
      mutate_at(vars(one_of(dummy_vars)), test_fun) %>%
      mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor)
    ret <- c(ret, T)
  } else {
    d_test <- d_test %>%
      mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor)
    ret <- c(ret, F)
    }
    d <- d_train %>% full_join(d_test) %>% arrange(Full_Date)
    d_split <- initial_time_split(d, prop = 0.75)
    d_train <- training(d_split)
    d_test  <- testing(d_split)
    save(d, file = sprintf("%s/04-data/02-model-data/%s_%s_%s_%s_%s.RData"
                         , res_path, SID, outcome, group, set, time))
    save(d_train, file = sprintf("%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData"
                         , res_path, SID, outcome, group, set, time))
    save(d_test, d_split, file = sprintf("%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData"
                         , res_path, SID, outcome, group, set, time))
    # return(T)
    if(any(ret == T)) ret <- T else ret <- F
    return(ret)
  # }
}

factor_fun <- function(x){if(is.numeric(x)){diff(range(x, na.rm = T)) %in% 1:2} else{F}}

dummy_vars <- c("o_value", "argument", "interacted", "lostSmthng"
                , "late", "frgtSmthng", "brdSWk", "excSWk", "AnxSWk"
                , "tired", "sick", "sleeping", "class"
                , "music", "internet", "TV", "study")
time_dummy <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
                , "morning", "midday", "evening", "night")

data_fun <- function(group, set, outcome, time){
  if(set != "all") groups <- set 
  if(set == "all") {
    if(group == "psychological") groups <- c("BFI-2", "Affect")
    else if(group == "situations") groups <- c("S8-I", "sit")
    else groups <- c("BFI-2", "Affect", "S8-I", "sit")
  }
  if(time == "time") groups <- c(groups, "time") 
  out <- all_features %>% 
    filter(trait == outcome) %>%
    select(SID, Full_Date, value) %>%
    group_by(SID) %>%
    mutate(o_value = lag(value)) %>% 
    ungroup() %>%
    select(-value)
  d <- all_features %>% 
    filter(category %in% groups) %>%
    mutate(name = ifelse(category == "BFI-2", paste(trait, facet, sep = "_"), trait)) %>%
    select(SID, Full_Date, name, value) %>%
    distinct() %>%
    pivot_wider(names_from = "name"
                , values_from = "value") %>%
    full_join(out) %>%
    filter(complete.cases(.))
    
  d %>% 
    group_by(SID) %>% 
    filter(n() >= 40) %>%
    nest() %>% 
    ungroup() %>%
    mutate(data = pmap(list(data, group, set, outcome, SID, time), possibly(save_fun, NA_real_))) %>%
    unnest(data)
}

nested_data <- tribble(
  ~group, ~set,
  "psychological", "BFI-2" , 
  "psychological", "Affect", 
  "psychological", "all"   , 
  "situations"   , "sit"   , 
  "situations"   , "S8-I"  , 
  "situations"   , "all"   , 
  "full"         , "all"   , 
) %>%
  full_join(crossing(group = c("psychological", "situations", "full")
                     , time = c("no time", "time")
                     , outcome = c("prcrst", "lonely"))) %>%
  mutate(data = pmap(list(group, set, outcome, time), possibly(data_fun, NA_real_)))
```

## Demographics  

```{r}
load(url(sprintf("%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData", res_path)))

dem <- baseline %>%
  select(SID:race) %>%
  mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB),
         StartDate = as.Date(ymd_hms(StartDate)),
         race = factor(race, 0:3, c("White", "Black", "Asian", "Other"))) %>%
  select(-YOB) 

 prelim_dem <- all_features %>%
   filter(category %in% c("Affect", "BFI-2", "sit", "SI-8")) %>%
   group_by(SID, Full_Date, trait, facet, category) %>%
   summarize(value = mean(value, na.rm = T)) %>%
   ungroup() %>%
   pivot_wider(names_from = c("category", "trait", "facet")
               , values_from = value) %>%
   filter(complete.cases(.)) 
 
 prelim_dem %>% 
   group_by(SID) %>%
   tally() %>% 
   ungroup() %>% 
   left_join(dem) %>%
  summarize(N = length(unique(SID)),
            n = sprintf("%.2f (%.2f; %i-%i", mean(n), sd(n), min(n), max(n)),
            gender = sprintf("%i (%.2f%%)",sum(gender == "Female", na.rm = T), sum(gender == "Female", na.rm = T)/n()*100),
            age = sprintf("%.2f (%.2f)", mean(age, na.rm = T), sd(age, na.rm = T)),
            white = sprintf("%i (%.2f%%)"
                            , sum(race == "White", na.rm = T)
                            , sum(race == "White", na.rm = T)/n()*100),
            black = sprintf("%i (%.2f%%)"
                            , sum(race == "Black", na.rm = T)
                            , sum(race == "Black", na.rm = T)/n()*100),
            asian = sprintf("%i (%.2f%%)"
                            , sum(race == "Asian", na.rm = T)
                            , sum(race == "Asian", na.rm = T)/n()*100),
            other = sprintf("%i (%.2f%%)"
                            , sum(race == "Other", na.rm = T)
                            , sum(race == "Other", na.rm = T)/n()*100),
            StartDate = sprintf("%s (%s - %s)", median(StartDate), 
                                min(StartDate), max(StartDate)))
 
 final_dem <- prelim_dem %>%
   group_by(SID) %>%
   filter(n() >= 40) %>%
   tally() %>% 
   ungroup() %>% 
   left_join(dem)
 
unique(ldply(str_split(list.files(sprintf("%s/05-results/01-glmnet/01-tuning-models", res_path)), pattern = "_"), function(x) x[1]))$V1
 
final_dem %>% 
  filter(SID %in% unique(ldply(str_split(list.files(sprintf("%s/05-results/01-glmnet/01-tuning-models", local_path)), pattern = "_"), function(x) x[1]))$V1) %>%
  summarize(N = length(unique(SID)),
            n = sprintf("%.2f (%.2f; %i-%i", mean(n), sd(n), min(n), max(n)),
            gender = sprintf("%i (%.2f%%)",sum(gender == "Female", na.rm = T), sum(gender == "Female", na.rm = T)/n()*100),
            age = sprintf("%.2f (%.2f)", mean(age, na.rm = T), sd(age, na.rm = T)),
            white = sprintf("%i (%.2f%%)"
                            , sum(race == "White", na.rm = T)
                            , sum(race == "White", na.rm = T)/n()*100),
            black = sprintf("%i (%.2f%%)"
                            , sum(race == "Black", na.rm = T)
                            , sum(race == "Black", na.rm = T)/n()*100),
            asian = sprintf("%i (%.2f%%)"
                            , sum(race == "Asian", na.rm = T)
                            , sum(race == "Asian", na.rm = T)/n()*100),
            other = sprintf("%i (%.2f%%)"
                            , sum(race == "Other", na.rm = T)
                            , sum(race == "Other", na.rm = T)/n()*100),
            StartDate = sprintf("%s (%s - %s)", median(StartDate), 
                                min(StartDate), max(StartDate)))
```


```{r}
rm(list = ls()[!ls() %in% c("codebook", "ipcs_codebook", "res_path", "local_path", "sheets", "outcomes", "ftrs")])
```


<!--chapter:end:01-data-cleaning.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Elastic Net  
Elastic Net Regression (ENR) proceeds from the observation that typical OLS-based regression minimizes bias but may have great variance. Using L1 (Ridge) and L2 (LASSO) approaches, which apply penalties to model estimates, ENR attempts to balance the trade-off between bias and variance by choosing the best penalties that minimize an information criterion or prediction error. Together, these both shrink coefficients and help with feature selection by forcing some of the coefficients to be zero. Because there are a large number of values the regularization parameter λ can take on, the typical solution is to use a cross-validation to test a number of possible $\lambda$ penalty values and choose the one with the one that matches a criterion like minimizing prediction error.  

In the present study, we used the `tidymodels` package in R to estimate the ENR models by calling the `logistic_reg()`, setting the engine as `“glmnet”`, and the mode as `“classification”`. The parameters tuned via rolling origin forecast validation were penalty and mixture, which were each set to 10 values. Next, we used the `select_best()` function with the method set to `“accuracy”` to allow the algorithm to automatically pick the best combination of penalty and mixture that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of penalty and mixture and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the `collect_metrics()` function.

## Functions  
```{r, eval = F}
dummy_vars <- c("o_value", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
                , "morning", "midday", "evening", "night", "argument"
                , "interacted", "lostSmthng", "late", "frgtSmthng", "brdSWk"
                , "excSWk", "AnxSWk", "tired", "sick", "sleeping", "class"
                , "music", "internet", "TV", "study")

time_vars <- c("sin2p", "sin1p", "cos2p", "cos1p")

c_fun <- function(m){
  # final model characteristics
  lambda <- min(m$fit$lambda)
  coefs <- stats::coef(m$fit, s = lambda)
  coefs <- coefs[, 1L, drop = TRUE]
  coefs <- coefs[setdiff(x = names(coefs), y = "(Intercept)")]
  return(coefs)
}

elnet_fun <- function(sid, outcome, group, set, time){
  # load the data
  load(sprintf("%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  d_train <- d_train %>% arrange(Full_Date) %>% select(-Full_Date) #%>%
    # mutate_at(vars(-o_value), ~as.numeric(as.character(.)))
  init <- ceiling(nrow(d_train)/3)
  
  d_train_cv <- rolling_origin(
    d_train, 
    initial = init,
    assess = 5,
    skip = 1,
    cumulative = TRUE
  )
  
  no_zv_fun <- function(x){
    if(is.numeric(x)) sd(x, na.rm = T) != 0 else length(table(x)) > 1
  }
  
  # set up the cross-valiation folds
  # set.seed(234)
  # d_train_cv <- vfold_cv(d_train, v = 10)
  
  # set up the data and formula
  mod_recipe <- recipe(
    o_value ~ .
    , data = d_train
    ) %>%
    step_zv(all_predictors(), -all_outcomes()) %>%
    step_nzv(all_predictors(), unique_cut = 35) %>%
    step_dummy(one_of(dummy_vars), -all_outcomes()) %>%
    step_normalize(-one_of(dummy_vars), -one_of(time_vars)) #%>%
    # estimate the means and standard deviations
    # prep(training = d_train, retain = TRUE)
  
  # set up the model specifications 
  tune_spec <- 
    logistic_reg(
      penalty = tune()
      , mixture = tune()
    ) %>% 
    set_engine("glmnet") %>% 
    set_mode("classification")
  
  # set up the ranges for the tuning functions 
  elnet_grid <- grid_regular(penalty()
                             , mixture()
                             , levels = 10)
  # set up the workflow: combine modeling spec with modeling recipe
  set.seed(345)
  elnet_wf <- workflow() %>%
    add_model(tune_spec) %>%
    add_recipe(mod_recipe)

  # combine the workflow, and grid to a final tuning model
  elnet_res <-
    elnet_wf %>%
    tune_grid(
      resamples = d_train_cv
      , grid = elnet_grid
      , control = control_resamples(save_pred = T)
      )
  save(elnet_res, file = sprintf("%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # load(sprintf("%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s.RData",
  #              res_path, sid, outcome, group, set))

  # plot the metrics across tuning parameters
  p <- elnet_res %>%
    collect_metrics() %>%
      ggplot(aes(penalty, mean, color = mixture)) +
      geom_point(size = 2) +
      facet_wrap(~ .metric, scales = "free", nrow = 2) +
      scale_x_log10(labels = scales::label_number()) +
      scale_color_gradient(low = "gray90", high = "red") +
      theme_classic()
  ggsave(p, file = sprintf("%s/05-results/01-glmnet/02-tuning-figures/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 8)
  
  # load(sprintf("%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData",
  #              res_path, sid, outcome, group, set, time))
  
  # select the best model based on AUC
  best_elnet <- elnet_res %>%
    # select_best("roc_auc")
    select_best("accuracy")
  
  # set up the workflow for the best model
  final_wf <- 
    elnet_wf %>% 
    finalize_workflow(best_elnet)
  
  # run the final best model on the training data and save
  final_elnet <- 
    final_wf %>%
    fit(data = d_train) 
  
    
  final_m <- final_elnet %>% 
    pull_workflow_fit() 
  final_coefs <- c_fun(final_m)
  
  best_elnet <- best_elnet %>%
    mutate(nvars = length(final_coefs[final_coefs != 0]))
  
  save(final_coefs, best_elnet,
       file = sprintf("%s/05-results/01-glmnet/07-final-model-param/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # load the test data
  load(sprintf("%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  # d_split$data$o_value <- factor(d_split$data$o_value)
  # d_split$data<-d_split$data %>% 
  #   mutate_at(vars(-Full_Date, -o_value), ~as.numeric(as.character(.)))
  
  # run the final fit workflow of the training and test data together
  final_fit <- 
    final_wf %>%
    last_fit(d_split) 
  save(final_elnet, final_fit
       , file = sprintf("%s/05-results/01-glmnet/03-final-training-models/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # final metrics (accuracy and roc)
  final_metrics <- final_fit %>%
    collect_metrics(summarize = T)
  save(final_metrics
       , file = sprintf("%s/05-results/01-glmnet/06-final-model-performance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # variable importance
  final_var_imp <- final_elnet %>% 
    pull_workflow_fit() %>% 
    vi() %>%
    slice_max(Importance, n = 10)
  save(final_var_imp
       , file = sprintf("%s/05-results/01-glmnet/05-variable-importance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # roc plot
  p_roc <- final_fit %>%
    collect_predictions() %>% 
    roc_curve(.pred_0, truth = o_value) %>% 
    autoplot() + 
    labs(title = sprintf("Participant %s: %s, %s, %s, %s"
                         , sid, outcome, group, set, time)) 
  ggsave(p_roc, file = sprintf("%s/05-results/01-glmnet/04-roc-curves/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 5)
  
  rm(list = c("final_var_imp", "final_metrics", "final_wf", "final_elnet", "final_fit"
              , "best_elnet", "elnet_res", "elnet_wf", "elnet_grid", "tune_spec", "mod_recipe"
              , "p", "p_roc", "d_split", "d_test", "d_train", "d_train_cv"))
  gc()
  return(T)
}
```

## Run Models  
```{r, eval = F}
plan(multisession(workers = 12L))
elnet_mods <- tibble(
  file = sprintf("%s/04-data/03-train-data", res_path) %>% list.files()
) %>%
  separate(file, c("SID", "outcome", "group", "set", "time"), sep = "_") %>%
  mutate(time = str_remove_all(time, ".RData")) %>%
  mutate(mod = 
           future_pmap(
           list(SID, outcome, group, set, time)
           , safely(elnet_fun, NA_real_)
           , .progress = T
           , .options = future_options(
             globals = c("res_path", "dummy_vars", "c_fun", "time_vars")
             , packages = c("plyr", "tidyverse", "glmnet", "tidymodels", "vip")
           )
           )
         ) 
closeAllConnections()
```

<!--chapter:end:02-elastic-net.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# BISCWIT{#biscwit}  

The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT) is a correlation-based machine learning technique. The technique, which we modified to be compatible with rolling origin validation rather than k-fold cross validation as implemented in the `best.scales()` function in the `psych` package in `R`, proceeds as follows. First, pairwise correlations between predictors and outcome(s) are calculated using rolling origin forecast validation where the number of best items is varied. Second, the number of items in the final model is determined by finding the average correlation across the rolling origin training sets with the validation sets for each number of items and choosing the one with the highest average correlation. Third, the final training model is constructed by running the procedure on the full training set with the number of items determined by the validation procedure. Fourth, weighted sum scores of the features for both the training and test sets are extracted.  

Finally, accuracy and AUC are calculated. Accuracy was estimated by (1) scaling the scores in the test set by the mean and standard deviation of scores in the final training model, (2) multiplying the scaled scores by the correlation between the training scores and the training outcome and the standard deviation of the training outcome and adding the mean of the training data outcome to this. This resulted in predicted outcomes for the test set. As in other classification models, values of .5 and above were considered to predict a “1” while values less than .5 were considered to predict a “0.” Accuracy was determined by comparing the predicted value with the actual test value and averaging the number correct. AUC was calculated using the predicted values and the test set values for the outcome.  

In the present study, we will use the `bestScales()` function from the psych package to create the models. Weighted scores were calculated by extracting the correlations from the best scales object and using it in the `scoreWtd()` function to create the correlation weighted scores. AUC was calculated using the `vi()` function in the vip package. Variable importance was determined by the items with the highest correlation with the outcome in the final training model.


## Functions  
```{r, eval = F}
biscwit_call <- function(x, nitem){
  ## format the ro training set
  x_train <- training(x) %>% 
    select(o_value, everything()) %>%
    mutate_if(is.factor, ~as.numeric(as.character(.))) %>%
    unclass() %>%
    data.frame()
  
  ## call the best scales function with nitem set from outer loop
  bs <- bestScales(
    x = x_train
    , criteria = "o_value"
    , n.item = nitem
    , n.iter = 1
  )
  
  ## get the multiple R's (weights for biscwit v biscuit)
  # the second line sets the items not found by biscuit to be "best" 
  # to be 0
  mR <- bs$R
  mR[!names(mR) %in% str_remove(bs$best.keys$o_value, "-")] <- 0
  mR <- as.matrix(mR); colnames(mR) <- "o_value"
  
  ## format the test set
  x_test <- testing(x) %>% 
    select(o_value, everything()) %>%
    mutate_if(is.factor, ~as.numeric(as.character(.))) %>% 
    # select(one_of(names(mR))) %>%
    unclass() %>%
    data.frame()
  
  ## score the test set using the correlations as weights
  scores <- scoreWtd(mR, x_test)
  ## calculate the correlation between the weighted score and the test y
  r <- cor(scores, as.numeric(as.character(testing(x)$o_value)))
  
  ## return the biscuit object and the test correlations (criterion)
  return(list(bs = bs, r = r))
}

biscwit_fun <- function(sid, outcome, group, set, time){
  # load the data
  load(sprintf("%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  ## format the training data
  x_train <- d_train %>% 
    arrange(Full_Date) %>% 
    select(-Full_Date) %>% 
    select(o_value, everything()) %>%
    mutate_if(is.factor, ~as.numeric(as.character(.))) %>%
    unclass() %>%
    data.frame()
  
  ## create the rolling_origin training and validation sets
  init <- ceiling(nrow(d_train)/3)
  
  d_train_cv <- rolling_origin(
    x_train, 
    initial = init, 
    assess = 5,
    skip = 1,
    cumulative = TRUE
  )
  
  ## run biscwit on the ro
  ## run for nitem = 3 to the number of x training columns in increments of 3
  tune_res <- d_train_cv %>%
    mutate(nitem = map(splits, ~seq(3, ncol(training(.)) - 1, 3))) %>%
    unnest(nitem) %>%
    mutate(cv = map2(splits, nitem, possibly(biscwit_call, NA_real_))) %>% ## run the biscuit procedure
    # mutate(cv = map2(splits, nitem, biscwit_call)) %>% ## run the biscuit procedure
    filter(!is.na(cv)) %>%
    mutate(error = map_dbl(cv, ~(.)$r)) ## correlation is the training "error" (higher = lower error)
  save(tune_res, file = sprintf("%s/05-results/02-biscwit/01-tuning-models/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # plot the metrics across tuning parameters
  
  p <- tune_res %>%
    group_by(nitem) %>%
    summarize(merror = fisherz2r(mean(fisherz(error), na.rm = T))) %>%
    arrange(desc(abs(merror))) %>%
    ggplot(aes(x = nitem, y = merror)) +
      scale_alpha_continuous(range=c(.4,1))+
      geom_point(aes(alpha = abs(merror)), color = "red") +
      labs(x = "n items", y = "Correlation", "Correlation Strength (absolute value)") +
      theme_classic()
  ggsave(p, file = sprintf("%s/05-results/02-biscwit/02-tuning-figures/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 5)
  
  # find the best model by averaging correlations across ro sets 
  # and choosing the one with the highest average correlation
  best_biscwit <- tune_res %>%
    group_by(nitem) %>%
    summarize(merror = fisherz2r(mean(fisherz(error), na.rm = T))) %>%
    arrange(desc(abs(merror))) %>%
    slice_head(n = 1)
  
  ## rerun biscuit on the full training set with the best nitem
  final_biscwit <- bestScales(
    x = x_train
    , criteria = "o_value"
    , n.item = best_biscwit$nitem
    , n.iter = 1
  )
  
  save(final_biscwit
       , file = sprintf("%s/05-results/02-biscwit/03-final-training-models/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # get the correlations for weighting
  # the second line sets the items not found by biscuit to be "best" 
  # to be 0
  mR <- final_biscwit$R; mR <- mR[!names(mR) == "o_value"]
  mR[!names(mR) %in% str_remove(final_biscwit$best.keys$o_value, "-")] <- 0
  mR <- as.matrix(mR); colnames(mR) <- "o_value"
  
  final_m <- final_biscwit
  final_coefs <- mR
  
  best_biscwit <- best_biscwit %>%
    mutate(nvars = length(mR[mR != 0]))
  
  save(final_coefs, best_biscwit,
       file = sprintf("%s/05-results/02-biscwit/07-final-model-param/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # load the test data
  load(sprintf("%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  ## format the test set
  x_test <- d_test %>% 
    arrange(Full_Date) %>% 
    select(o_value, everything(), -Full_Date) %>%
    mutate_if(is.factor, ~as.numeric(as.character(.))) %>%
    unclass() %>%
    data.frame()
  
  # adapted from Elleman, McDougald, Condon, & Revelle (2020)
  # Step 1. Score the train X.
  scores_best <- scoreWtd(mR, x_train)
  
  # Step 2. Score the test X.
  scores_best_test <- scoreWtd(mR, x_test)
  
  # Step 3. Get the diagnostic info from the train y's scoring.
  cor(scores_best, x_train$o_value, use = "pairwise")
  ## mean and sd of training scores for observations 1 to t.
  mean_biscuit <- mean(scores_best, na.rm = T)
  sd_biscuit   <- sd(  scores_best, na.rm = T)
  
  # performance in training set
  multiple_R <- cor(scores_best, x_train$o_value, use = "pairwise") 
  
  # descriptives of the outcome variable in the training set
  mean_data <- mean(x_train$o_value, na.rm = T)
  sd_data   <- sd(  x_train$o_value, na.rm = T)
  
  # Step 4. Apply the diagnostic info to the test y's scoring.
  # Standardardize scores in the test set using the training set
  # subtract average training score from each score in the test set and divide by SD
  scores_best.z_test <- c((scores_best_test - mean_biscuit) / sd_biscuit)
  # scores_best.z_test_vector <- c(scores_best.z_test)
  
  # reverse transform using mean and SD of outcome in the training set
  # and the correlation between scores and outcome in the training set
  value_test <- ( c(multiple_R) * scores_best.z_test * sd_data ) + mean_data
        
  # Step 5. You have your predicted test values. Look at them to make sure they make sense!!
  temp <- data.frame(crit_actual = x_test$o_value, crit_predicted = value_test)
  
  # Get the accuracy. Since the reverse standardization should make these raw, 
  # we'll just essentially round these values to 0 or 1 (whichever is closer)
  # to determine accuracy
  temp_diagnostics <- as_tibble(temp) %>%
    mutate(
      accuracy = ifelse((crit_predicted >= .5 & crit_actual == 1) |  
                        (crit_predicted < .5 & crit_actual == 0), 1, 0)
    )
  
  # Best model results, testing data
  ## Getting and saving final metrics
  ## First get average accuracy in the test set
  value_accuracy <- mean(temp_diagnostics$accuracy, na.rm = T)
  ## Next get AUC's. Building in a failsafe in case in the case of no variance 
  ## in the test set, which would result in an error and break the code
  ## formatted to match output from tidymodels collect_metrics() for consistency
  if(sd(temp$crit_actual) != 0){
    roc <- roc_auc(temp %>% mutate(crit_actual = as.factor(crit_actual))
                   , truth = crit_actual
                   , crit_predicted)
  } else {
    roc <- tibble(
      .metric = "roc_auc"
      , .estimator = "binary"
      , .estimate = NA_real_
    )
  }
  
  ## reformat accuracy and join with AUC
  final_metrics <- roc %>%
    bind_rows(tibble(
      .metric = "accuracy"
      , .estimator = "binary"
      , .estimate = value_accuracy))
  
  save(final_metrics
       , file = sprintf("%s/05-results/02-biscwit/06-final-model-performance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  ## variable importance
  ## reformatted to match the output of the vi() function in the vip package
  final_var_imp <- data.frame(Importance = final_biscwit$R) %>%
    rownames_to_column("Variable") %>%
    mutate(Sign = ifelse(sign(Importance) == 1, "POS", "NEG")) %>%
    as_tibble() %>%
    arrange(desc(abs(Importance))) %>%
    slice_max(abs(Importance), n = 10) 
  save(final_var_imp
       , file = sprintf("%s/05-results/02-biscwit/05-variable-importance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  ## roc curve of test data
  p_roc <- roc_curve(
    temp %>% mutate(crit_actual = as.factor(crit_actual))
    , truth = crit_actual
    , crit_predicted
    )  %>%
    autoplot() +
    labs(title = sprintf("Participant %s: %s, %s, %s, %s"
                         , sid, outcome, group, set, time)) 
  ggsave(p_roc, file = sprintf("%s/05-results/02-biscwit/04-roc-curves/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 5)
  
  rm(list = ls())
  gc()
  return(T)
}
```

## Run Models  
```{r, eval = F}
done <- tibble(file = list.files(sprintf("%s/05-results/02-biscwit/06-final-model-performance", res_path)), done = "done")

plan(multisession(workers = 12L))
res <- tibble(
  file = sprintf("%s/04-data/02-model-data", res_path) %>% list.files()
) %>%
  separate(file, c("SID", "outcome", "group", "set", "time"), sep = "_") %>%
  mutate(time = str_remove_all(time, ".RData")
         , mod = future_pmap(
           list(SID, outcome, group, set, time)
           , possibly(biscwit_fun, NA_real_)
           , .progress = T
           , .options = future_options(
             globals = c("res_path", "biscwit_call")
             , packages = c("plyr", "tidyverse", "psych", "tidymodels", "vip")
           )
           )
         ) 
closeAllConnections()
```

<!--chapter:end:03-biscwit.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Random Forest  

Random forest models are a variant of decision tree classification algorithms that additionally draw on bagging (i.e. ensemble methods; bootstrapping with aggregation) methods. The name itself hints at how it different from classic decision trees; we have a forest instead of a tree. In so doing, random forest models draw upon a large number of decision trees of varying depth that are then aggregated. The random part comes from two sources. First, each tree in the forest in trained on a data set drawn with replacement from the training dataset (i.e. bootstrapped) as part of the bagging procedure. Observations left out of each model are termed out-of-bag observations (and collectively, the out-of-bag dataset). These are used to evaluate the performance of the tree. Second, the features used in each tree are also randomly drawn from the full of features. Each of these trees, based on their data generates a prediction given new data. The final prediction is based off the ensemble of trees – that is, the decision made by a majority of the trees (i.e. aggregation). Importantly, because of the random feature selection part of the procedure, random forest can also provide estimates of variable importance, indicating which features are critical to making a less error-prone classification.  

Because random forest using bagging (i.e. bootstrapping with aggregation), we will have to perform a series of steps that make bootstrapping appropriate with time series data: differencing, Box-Cox transformations, and time-delay embedding. Essentially, differencing and Box-Cox transformations stabilize the mean and variance, respectively, to make the time series stationary (Priestley, 1988), and time-delay embedding quite literally embeds the sequence of the time series into predictor variables, which in effect preserves the order of the times series (Von Oertzen & Boker, 2010). We can easily back transform forecasts to their original scale.  

In the present study, we used the `tidymodels` package in R to estimate the random forest models by calling the `rand_forest()`, setting the engine as `“ranger”`, with `importance = “permutation”` in order to extract variable importance, and the mode as `“classification”`. The parameters tuned via rolling origin forecast validation were `mtry` (i.e. the number of predictors that will be randomly sampled at each split when creating tree models) and `min_n` (i.e. the minimum number of data points in a node that is required for the node to be split further), which were each set to 10 values. Next, we used the `select_best()` function with the `method` set to `“accuracy”` to allow the algorithm to automatically pick the best combination of `mtry` and `min_n` that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of `mtry` and `min_n` and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the `collect_metrics()` function.  

## Set Up Data  
### Differencing and Box Cox  
```{r, eval = F}
dummy_vars <- c("o_value", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
                , "morning", "midday", "evening", "night", "argument"
                , "interacted", "lostSmthng", "late", "frgtSmthng", "brdSWk"
                , "excSWk", "AnxSWk", "tired", "sick", "sleeping", "class"
                , "music", "internet", "TV", "study")

time_vars <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
               , "morning", "midday", "evening", "night"
               , "sin2p", "sin1p", "cos2p", "cos1p"
               , "cub", "linear", "quad")


rf_fun <- function(sid, outcome, group, set, time){
  load(sprintf("%s/04-data/02-model-data/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  # differencing and box-cox
  d <- d %>%
    mutate_if(is.factor, ~as.numeric(as.character(.))) %>%
    mutate_at(vars(-one_of(c(dummy_vars, time_vars)), -Full_Date), log) %>% 
    mutate_at(vars(-one_of(c(dummy_vars, time_vars)), -Full_Date), ~. - lag(.))
  # time delay embedding
  d_mbd <- map(d %>% select(-Full_Date, -one_of(time_vars)), ~embed(., 2)) %>% ldply(.) %>%
    group_by(.id) %>%
    mutate(beep = 1:n()) %>%
    ungroup() %>%
    pivot_wider(names_from = ".id"
                , names_glue = "{.id}_{.value}"
                , values_from = c("1", "2")) %>%
    bind_cols(d[-1,] %>% select(Full_Date)) %>%
    select(-beep)
  
  d_mbd <- d_mbd %>% 
    full_join(d %>% select(Full_Date, one_of(time_vars))) %>%
    mutate_at(vars(contains(dummy_vars)), factor) %>%
    # mutate(o_value_1 = factor(o_value_1)) %>%
    drop_na()
  
  # training and test sets
  d_split <- initial_time_split(d_mbd, prop = 0.75)
  d_train <- training(d_split)
  d_test  <- testing(d_split)
  
  d_train <- d_train %>% arrange(lubridate::ymd_hm(Full_Date)) %>% select(-Full_Date)
  
  ## create the rolling_origin training and validation sets
  init <- ceiling(nrow(d_train)/3)
  
  # set up the cross-valiation folds
  d_train_cv <- rolling_origin(
    d_train, 
    initial = init, 
    assess = 5,
    skip = 1,
    cumulative = TRUE
  )
  
  # set up the data and formula
  mod_recipe <- recipe(
    o_value_1 ~ .
    , data = d_train
    ) %>%
    step_zv(all_numeric(), contains(dummy_vars), contains(time_vars)) %>%
    step_dummy(all_nominal(), -all_outcomes()) %>%
    step_nzv(all_predictors(), unique_cut = 35) #%>%
    # estimate the means and standard deviations
    # prep(training = d_train, retain = TRUE)
  
  # set up the model specifications 
  tune_spec <- 
    rand_forest(
      mtry = tune()
      , trees = 1000
      , min_n = tune()
    ) %>% 
    set_engine("ranger", importance = "permutation") %>%
    set_mode("classification")
  
  # set up the workflow: combine modeling spec with modeling recipe
  set.seed(345)
  rf_wf <- workflow() %>%
    add_model(tune_spec) %>%
    add_recipe(mod_recipe)
  
  # set up the ranges for the tuning functions 
  set.seed(345)
  tune_res <- tune_grid(
    rf_wf
    , resamples = d_train_cv
    , grid = 20
  )
  save(tune_res, file = sprintf("%s/05-results/03-rf/01-tuning-models/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # load(sprintf("%s/05-results/03-rf/01-tuning-models/%s_%s_%s_%s_%s.RData",
  #              res_path, sid, outcome, group, set, time))
  
  # plot the metrics across tuning parameters
  p <- tune_res %>%
    collect_metrics() %>%
      ggplot(aes(mtry, mean, color = min_n)) +
      geom_point(size = 2) +
      facet_wrap(~ .metric, scales = "free", nrow = 2) +
      scale_x_log10(labels = scales::label_number()) +
      scale_color_gradient(low = "gray90", high = "red") +
      theme_classic()
  ggsave(p, file = sprintf("%s/05-results/03-rf/02-tuning-figures/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 8)
  
  # select the best model based on AUC
  best_rf <- tune_res %>%
    # select_best("roc_auc")
    select_best("accuracy")
  
  # set up the workflow for the best model
  final_wf <- 
    rf_wf %>% 
    finalize_workflow(best_rf)
  
  # run the final best model on the training data and save
  final_rf <- 
    final_wf %>%
    fit(data = d_train) 
  
  final_m <- final_rf %>% 
    pull_workflow_fit() 
  final_coefs <- final_m$fit$variable.importance
  
  best_rf <- best_rf %>%
    mutate(nvars = length(final_coefs[final_coefs != 0]))
  
  save(final_coefs, best_rf,
       file = sprintf("%s/05-results/03-rf/07-final-model-param/%s_%s_%s_%s_%s.RData",
               res_path, sid, outcome, group, set, time))
  
  # run the final fit workflow of the training and test data together
  final_fit <- 
    final_wf %>%
    last_fit(d_split) 
  save(final_rf, final_fit
       , file = sprintf("%s/05-results/03-rf/03-final-training-models/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # final metrics (accuracy and roc)
  final_metrics <- final_fit %>%
    collect_metrics(summarize = T)
  save(final_metrics
       , file = sprintf("%s/05-results/03-rf/06-final-model-performance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # variable importance
  final_var_imp <- final_rf %>% 
    pull_workflow_fit() %>% 
    vi() %>%
    slice_max(Importance, n = 10)
  save(final_var_imp
       , file = sprintf("%s/05-results/03-rf/05-variable-importance/%s_%s_%s_%s_%s.RData",
                        res_path, sid, outcome, group, set, time))
  
  # roc plot
  p_roc <- final_fit %>%
    collect_predictions() %>% 
    roc_curve(.pred_0, truth = o_value_1) %>% 
    autoplot() + 
    labs(title = sprintf("Participant %s: %s, %s, %s, %s"
                         , sid, outcome, group, set, time)) 
  ggsave(p_roc, file = sprintf("%s/05-results/03-rf/04-roc-curves/%s_%s_%s_%s_%s.png",
               res_path, sid, outcome, group, set, time)
         , width = 5, height = 5)
  
  rm(list = c("final_var_imp", "final_metrics", "final_wf", "final_rf", "final_fit"
              , "best_rf", "tune_res", "rf_wf", "tune_spec", "mod_recipe"
              , "p", "p_roc", "d_split", "d_test", "d_train", "d_train_cv"))
  gc()
  return(T)
  
}
```

### Run Models  
```{r, eval = F}
plan(multisession(workers = 12L))
rf_res <- tibble(
  file = sprintf("%s/04-data/02-model-data", res_path) %>% list.files()
) %>%
  separate(file, c("SID", "outcome", "group", "set", "time"), sep = "_") %>%
  mutate(time = str_remove_all(time, ".RData")) %>%
  mutate(
    mod = future_pmap(
           list(SID, outcome, group, set, time)
           , safely(rf_fun, NA_real_)
           , .progress = T
           , .options = future_options(
             globals = c("res_path", "dummy_vars", "time_vars")
             , packages = c("plyr", "tidyverse", "glmnet", "tidymodels", "vip")
           )
           )
         ) 
closeAllConnections()
```

<!--chapter:end:04-random-forest.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Summarizing Models  
Now that all the models have been run, the next step is to take various metrics and results from the models and format them into tables and figures that are more understable than thousands of model objects. 

## Question 1: Can we predict procrastination and loneliness?  

### Performance Metrics  
To begin, we'll pull the performance metrics -- classification accuracy and area under the receiver operating curve (AUC) -- to determine and display:  

1. Overall Model Performance  
2. Participant Specific Model Performance (e.g., did certain feature sets perform differently)  
3. Participants best models (in terms of accuracy and AUC) along with summaries of such accuracy and AUC, the feature set, etc.  

The first thing we need to do is load in the final model performance metrics -- that is, the accuracy and AUC of the model chosen via rolling-origin validation on the test / holdout set.  

```{r, eval = F}
loadRData <- function(fileName, type, model){
#loads an RData file, and returns it
    path <- sprintf("%s/05-results/%s/06-final-model-performance/%s", local_path, model, fileName)
    load(path)
    get(ls()[grepl(type, ls())])
}

sum_res <- tibble(
  model = c("01-glmnet", "02-biscwit", "03-rf")
) %>%
  mutate(file = map(model, ~sprintf("%s/05-results/%s/06-final-model-performance", local_path, .) %>%
                      list.files())) %>%
  unnest(file) %>%
  mutate(data = map2(file, model, ~loadRData(.x, "final_metrics", .y))) %>%
  separate(file, c("SID", "outcome", "group", "set", "time"), sep = "_") %>%
  mutate(time = str_remove_all(time, ".RData")
         , model = str_remove_all(model, "[0-9 -]"))
save(sum_res, file = sprintf("%s/05-results/final-model-performance.RData", local_path))
```

Which looks something like this:  

```{r}
load(url(sprintf("%s/05-results/final-model-performance.RData", res_path)))
sum_res %>%
  unnest(data)
```


```{r, eval = F}
loadRData <- function(fileName, type, model){
#loads an RData file, and returns it
    path <- sprintf("%s/05-results/%s/07-final-model-param/%s", res_path, model, fileName)
    load(path)
    get(ls()[grepl(type, ls())])
}

param_res <- tibble(
  model = c("01-glmnet", "02-biscwit", "03-rf")
) %>%
  mutate(file = map(model, ~sprintf("%s/05-results/%s/07-final-model-param", res_path, .) %>%
                      list.files())) %>%
  unnest(file) %>%
  mutate(params = map2(file, model, ~loadRData(.x, "best", .y))
         , coefs = map2(file, model, ~loadRData(.x, "coef", .y))) %>%
  separate(file, c("SID", "outcome", "group", "set", "time"), sep = "_") %>%
  mutate(time = str_remove_all(time, ".RData")
         , model = str_remove_all(model, "[0-9 -]")) 
save(param_res, file = sprintf("%s/05-results/final-model-param.RData", local_path))
```

Which looks like this:  

```{r}
load(url(sprintf("%s/05-results/final-model-param.RData", res_path)))
param_res
```


### Classification Accuracy and AUC for all Models   
Now that we've loaded in the results, the first thing that we'll do is create tables on the performance of each model for *all* the tested feature sets. The goal here is less to make any specific argument with the results and more to just document them in a nice table format that is easier to read.  

```{r}
perf_tab_fun <- function(d, outcome, group, set, time){
  # format groups, time, and outcomes to nice names
  g <- str_to_title(group); s <- str_to_title(set)
  tm <- if(time == "time") "With Time" else "Without Time"
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  # create the caption
  cap <- sprintf("<strong>Table SX</strong><br><em>Performance Metrics of the %s (%s) Feature Set %s Predicting %s", g, s, tm, o)
  # call kable to create the html table
  tab <- d %>%
    kable(.
          , "html"
          , col.names = c("ID", rep(c("Accuracy", "AUC"), times = 3))
          , align = c("r", rep("c", 6))
          , digits = 2
          , caption = cap
    ) %>%
    kable_styling(full_width = F) %>%
    add_header_above(c(" " = 1, "Elastic Net" = 2, "BISCWIT" = 2, "Random Forest" = 2))
  # save the table to files
  save_kable(tab, file = sprintf("%s/05-results/04-tables/01-participant-metrics/%s_%s_%s_%s.html", local_path, outcome, group, set, time))
  # return the table object
  return(tab)
}

sum_res_tab <- sum_res %>%
  unnest(data) %>%
  select(-.estimator, -.config) %>%
  pivot_wider(names_from = c("model", ".metric")
              , values_from = ".estimate") %>%
  group_by(outcome, group, set, time) %>%
  nest() %>%
  ungroup() %>%
  mutate(tab = pmap(list(data, outcome, group, set, time), perf_tab_fun))
sum_res_tab
```


Now I'll print the tables in different tabs below. Here, I'm only showing the set with combined features for each category for parsimony. The full results are in the online materials under 05-results/04-tables/01-participant-metrics.  

#### Procrastination {.tabset}  
As is clear in each of these tables, overall accuracy and AUC are quite high although there are quite stark individual differences in them across people. AUC tended to be lower than accuracy on average. However, at the individual level, there are some individuals who had AUC scores higher than accuracy.  

```{r}
tmp <- sum_res_tab %>% 
  filter(set == "all" & outcome == "prcrst") %>%
  mutate(group = sprintf("%s, %s", str_to_title(group), str_to_title(time)))

for(i in 1:nrow(tmp)){
  cat('  \n\n##### ', tmp$group[i], '\n\n  ', sep ="")
  tmp$tab[[i]] %>%
    scroll_box(height = "500px") %>%
    print()
}
```

#### Loneliness {.tabset}  
As with procrastination, the loneliness tables indicate that overall accuracy and AUC are quite high although there are quite stark individual differences in them across people. AUC tended to be lower than accuracy on average. However, at the individual level, there are some individuals who had AUC scores higher than accuracy.  

```{r, results='asis'}
tmp <- sum_res_tab %>% 
  filter(set == "all" & outcome == "lonely") %>%
  mutate(group = sprintf("%s, %s", str_to_title(group), str_to_title(time)))

for(i in 1:nrow(tmp)){
  cat('  \n\n##### ', tmp$group[i], '\n\n  ', sep ="")
  tmp$tab[[i]] %>%
    scroll_box(height = "750px") %>%
    print()
}

```

### Best Models  
Next, to get a more concise indication of how these models are performing, we will choose the best model in terms of accuracy and AUC for each participant, outcome, and model combination.  

```{r}
best_mods <- sum_res %>% 
  unnest(data) %>%
  filter(set == "all") %>%
  group_by(SID, outcome, .metric, model) %>%
  filter(!is.na(.estimate) & .estimate != 1) %>%
  arrange(desc(.estimate), model, group) %>%
  slice_head(n = 1) %>%
  ungroup(); best_mods
```

#### Participant Summaries (Table) {.tabset}  

Now that we have participants best models, the first thing we'll do is create a summary of just how well participants' best models actually performed. These Supplementary Tables will be split by outcome (procrastination, loneliness) and metric (accuracy, AUC) with each row giving details on which feature set was chosen for each method and what the accuracy or AUC for that method was.  

```{r}
px_bm_fun <- function(d, outcome, metric){
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  m <- if(metric == "accuracy") "Accuracy" else "AUC"
  cap <- sprintf("<strong>Table SX</strong><br><em>Feature Set and %s for Predicting %s for Each Participant's Best Model</em>", m, o)
  tab <- d %>%
    select(SID, contains("glmnet"), contains("biscwit"), contains("rf")) %>%
    kable(.
          , "html"
          , digits = 2
          , col.names = c("ID", rep(c("Feature Set", m), times = 3))
          , align = c("r", rep(c("l", "c"), times = 3))
          , cap = cap
          ) %>%
      kable_styling(full_width = F) %>%
      add_header_above(c(" " = 1, "Elastic Net" = 2, "BISCWIT" = 2, "Random Forest" = 2))
  save_kable(tab, file = sprintf("%s/05-results/04-tables/02-participant-best-models/%s_%s.html", local_path, outcome, metric))
  return(tab)
}

px_best_mods <- best_mods %>% 
  select(-.estimator, -.config, -set) %>%
  mutate_at(vars(group, time), str_to_title) %>%
  unite(group, group, time, sep = ", ") %>%
  pivot_wider(names_from = "model"
              , values_from = c("group", ".estimate")
              , names_glue = "{model}_{.value}") %>%
  group_by(outcome, .metric) %>%
  nest() %>%
  ungroup() %>%
  mutate(tab = pmap(list(data, outcome, .metric), px_bm_fun)); px_best_mods
```

Now, let's print each of these tables and see what they demonstrate.  

##### Procrastination, Accuracy  
Across people, accuracy was, on average (and median), very high. Accuracy estimates across models tended to be very similar for each person, which indicates that the models seemed to perform similarly.  

```{r, results = 'asis'}
px_best_mods$tab[[1]] %>%
    scroll_box(height = "750px")
```

##### Procrastination, AUC  
Like accuracy, AUC in predicting future procrastination was quite high across the full sample. However, unlike accuracy, AUC tended to vary within-person across models suggesting that the accuracy results may have been somewhat misleading in suggesting that the models seemed to perform similarly. To better understand how, however, we will have to examine the features in more detail, as we will later in Questions 4 and 5.  

```{r, results = 'asis'}
px_best_mods$tab[[2]] %>%
    scroll_box(height = "750px")
```

##### Loneliness, Accuracy  
Across people, accuracy in predicting future loneliness was, on average (and median), very high. Accuracy estimates across models tended to be very similar for each person, which indicates that the models seemed to perform similarly. There are some exceptions to this, but the magnitude of these differences remains relatively small (magnitude of about .1 at most).  

```{r, results = 'asis'}
px_best_mods$tab[[3]] %>%
    scroll_box(height = "750px")
```

##### Loneliness, AUC  
Like accuracy, AUC in predicting future loneliness was quite high across the full sample. However, unlike accuracy, AUC tended to vary within-person across models suggesting that the accuracy results may have been somewhat misleading in suggesting that the models seemed to perform similarly. To better understand how, however, we will have to examine the features in more detail, as we will later in Questions 4 and 5.  

```{r, results = 'asis'}
px_best_mods$tab[[4]] %>%
    scroll_box(height = "750px")
```

#### Classification Accuracy and AUC  
##### Table  

Similar to how we created tables for each outcome, feature set, and metric in the first section, we will next create a single, similar table for participants best models, summarizing the mean, standard deviation, median, and range for each outcome, method, and metric. In the manuscript, this will be summarized in a figure, but I'm still creating the table for ease of access.  

```{r}
bm_tab <- best_mods %>%
  group_by(model, outcome, .metric) %>%
  summarize_at(vars(.estimate), lst(mean, sd, median, min, max, n=~sum(!is.na(.)))) %>%
  ungroup() %>%
  mutate(sd = ifelse(sd < .01, "<.01", sprintf("%.2f", sd)),
         mean = sprintf("%.2f (%s)", mean, sd),
         range = sprintf("%.2f-%.2f", min, max),
         median = sprintf("%.2f", median),
         model = factor(model, levels = c("glmnet", "biscwit", "rf")),
         .metric = factor(.metric, c("accuracy", "roc_auc"), c("Accuracy", "AUC"))
         ) %>%
  select(-sd, -min, -max) %>%
  pivot_wider(names_from = "outcome"
              , values_from = c(mean, median, range, n)
              , names_glue = "{outcome}_{.value}") %>%
  arrange(model, .metric) %>%
  select(.metric, contains("lonely"), contains("prcrst")) %>%
  kable(.
        , "html"
        , escape = F
        , col.names = c("Metric", rep(c("<em>M</em> (<em>SD</em>)", "Median", "Range", "<em>N</em>"), times = 2))
        , align = c("r", rep("c",8))
        , cap = "<strong>Table X</strong><br><em>Descriptive Statistics of Model Performance Across of the Best Performing Model for Each Participant</em>"
        ) %>%
  kable_styling(full_width = F) %>%
  kableExtra::group_rows("Elastic Net", 1, 2) %>%
  kableExtra::group_rows("BISCWIT", 3, 4) %>%
  kableExtra::group_rows("Random Forest", 5, 6) %>%
  add_header_above(c(" " = 1, "Loneliness" = 4, "Procrastination" = 4)) %>%
  footnote("Accuracy = Classification accuracy; AUC = Area under the receiver operating characteristic (ROC) curve.")
save_kable(bm_tab, file = sprintf("%s/05-results/04-tables/01-best-summary.html", local_path))
bm_tab
```

There are a few key takeaways from this table. First, accuracy across all models and outcomes was quite high, with mean accuracy of .87 (Median .91 to .92) for loneliness and between .82 and .83 (Median .88 to .89) for procrastination. Similarly, AUC was also well above the .5 threshold with means ranging from .70 to .76 (Median .75 to .80) for loneliness and .69 to .70 (Median .70 to .75) for procrastination.  

##### Figure (Figure 1)  
Now, we'll create distributions of the performance (accuracy, AUC) of participants' best models and plot those along with the descriptive statistics that were created for the Supplementary Table in the previous section. This figure will become Figure 1 in the manuscript.  

```{r, fig.height=5, fig.width=12, fig.align='center', fig.cap="Histograms of classification accuracy and Area Under the Receiver Operator Curve (AUC) for participants’ best models."}
p_dist_fun <- function(d, outcome) {
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  d %>% 
    mutate(model = factor(model, c("glmnet", "biscwit", "rf")
                            , c("Elastic Net", "BISCWIT", "Random Forest"))
             , .metric = factor(.metric, c("accuracy", "roc_auc"), c("Accuracy", "AUC"))
             , group = factor(str_to_title(group))) %>%
      ggplot(aes(y = model, x = .estimate)) + 
        scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + 
        geom_density_ridges(aes(fill = model), alpha = .5) +
        stat_pointinterval() +
        geom_vline(aes(xintercept = .5), linetype = "dashed") +
        labs(x = NULL, y = NULL, title = o) + 
        facet_wrap(~.metric, scales = "free", nrow = 2) +
        theme_classic() + 
        theme(legend.position = "none"
              , axis.text = element_text(face = "bold")
              , axis.title = element_text(face = "bold")
              , strip.background = element_blank()
              , strip.text.y = element_blank()
              , plot.margin = margin(.1,.1,1,.1, unit = "cm")
              , strip.text = element_text(face = "bold", size = rel(1.2))
              , plot.title = element_text(face = "bold", size = rel(1.2), hjust = .5))
}

bm_dist <- best_mods %>% 
  group_by(outcome) %>%
  nest() %>%
  ungroup() %>%
  mutate(p = map2(data, outcome, p_dist_fun))

tab_fun <- function(d){
  tab <- d %>%
    select(-model) %>%
    setNames(c("M (SD)", "Median", "N", "Range")) %>%
    tableGrob(rows = NULL
              , theme = ttheme_minimal(base_family = "Times"))
  tab <- gtable_add_grob(tab,
        grobs = segmentsGrob( # line across the bottom
            x0 = unit(0,"npc"),
            y0 = unit(0,"npc"),
            x1 = unit(1,"npc"),
            y1 = unit(0,"npc"),
            gp = gpar(lwd = 2.0)),
        t = 1, b = 1, l = 1, ncol(tab))
  tab$grobs[1:4] <- lapply(tab$grobs[1:4], function(x) {x$grobs[[1]]$gp$fontface = "bold"; return(x)}) 
  return(tab)
}

bm_tbl <- best_mods %>%
  group_by(model, outcome, .metric) %>%
  summarize_at(vars(.estimate), lst(mean, sd, median, min, max, n=~sum(!is.na(.)))) %>%
  ungroup() %>%
  mutate(sd = ifelse(sd < .01, "<.01", sprintf("%.2f", sd)),
         mean = sprintf("%.2f (%s)", mean, sd),
         range = sprintf("%.2f-%.2f", min, max),
         median = sprintf("%.2f", median),
         model = factor(model, levels = c("glmnet", "biscwit", "rf"),
                        labels = c("Elastic Net", "BISCWIT", "Random Forest")),
         .metric = factor(.metric, c("accuracy", "roc_auc"), c("Accuracy", "AUC"))
         ) %>%
  select(-sd, -min, -max) %>%
  arrange(outcome, .metric, model) %>%
  group_by(outcome, .metric) %>% 
  nest() %>%
  ungroup() %>%
  mutate(tab = map(data, tab_fun))

my_theme <- function(...) {
  theme_classic() + 
    theme(plot.title = element_text(face = "bold"))
}

title_theme <- calc_element("plot.title", my_theme())

ttl <- ggdraw() + 
    draw_label(
        "Procrastination",
        fontfamily = title_theme$family,
        fontface = title_theme$face,
        size = title_theme$size
    )

bm_dist$p[[1]] <- bm_dist$p[[1]] + labs(title = NULL)

bm_tab1 <- plot_grid(bm_tbl$tab[[3]], bm_tbl$tab[[4]], nrow = 2, rel_heights = c(.4, .4))
bm_prcrst <- plot_grid(bm_dist$p[[1]], bm_tab1, ncol = 2)
bm_prcrst <- plot_grid(ttl, bm_prcrst, nrow = 2, rel_heights = c(.05,.95))

bm_dist$p[[2]] <- bm_dist$p[[2]] + labs(title = NULL) + theme(axis.text.y = element_blank())

ttl <- ggdraw() + 
    draw_label(
        "Loneliness",
        fontfamily = title_theme$family,
        fontface = title_theme$face,
        size = title_theme$size
    )

bm_tab2 <- plot_grid(bm_tbl$tab[[1]], bm_tbl$tab[[2]], nrow = 2)
bm_lonely <- plot_grid(bm_dist$p[[2]], bm_tab2, ncol = 2, rel_widths = c(.4, .6))
bm_lonely <- plot_grid(ttl, bm_lonely, nrow = 2, rel_heights = c(.05,.95))

bm_plot <- plot_grid(bm_prcrst, bm_lonely, ncol = 2, rel_widths = c(.55, .45)); bm_plot
ggsave(bm_plot, file = sprintf("%s/05-results/05-figures/fig-1-best-models.pdf", local_path)
       , width = 12, height = 5)
```

Figure 1 presents histograms and descriptive statistics of accuracy and AUC across the full sample for each outcome and model. As is clear in the figure, predictive accuracy was high overall, with mean accuracy of .87 (Median .91 to .92) for loneliness and between .82 and .83 (Median .88 to .89) for procrastination. Similarly, AUC was also well above the .5 threshold with means ranging from .70 to .76 (Median .75 to .80) for loneliness and .69 to .70 (Median .70 to .75) for procrastination.  


### Tuning Parameters (Table)   
Next, I'm going to create tables that include the tuning parameters, features, and accuracy for each participants best model for each machine learning method and outcome as well as which feature set was used in their best model.  
```{r}
px_tun_par_tab_fun <- function(d, model, outcome){
  if(model == "glmnet"){
    cn <- c("ID","Group", "Penalty", "Mixture", "# Features", "Accuracy")
    al <- c(rep("r", 2), rep("c", 4))
    tab <- d %>% 
      mutate(group = str_to_title(paste(group, time, sep = ", "))) %>%
      select(SID, group, penalty, mixture, nvars, .estimate) %>%
      mutate(penalty = ifelse(penalty < .01, sprintf("%.1e", penalty), sprintf("%.2f", penalty))
             , .estimate = sprintf("%.2f", .estimate)
             , mixture = ifelse(mixture == 0, "0", sprintf("%.2f", mixture)))
  } else if(model == "rf"){
    cn <- c("ID","Group", "# Features Sampled", "Min N for Split", "# Features", "Accuracy")
    al <- c(rep("r", 2), rep("c", 4))
    tab <- d %>% 
      mutate(group = str_to_title(paste(group, time, sep = ", "))) %>%
      select(SID, group, mtry, min_n, nvars, .estimate) %>%
      mutate(.estimate = sprintf("%.2f", .estimate))
  } else {
    cn <- c("ID","Group", "# Items", "# Features", "Accuracy")
    al <- c(rep("r", 2), rep("c", 3))
    tab <- d %>% 
      mutate(group = str_to_title(paste(group, time, sep = ", "))) %>%
      select(SID, group, nitem, nvars, .estimate) %>%
      mutate(.estimate = sprintf("%.2f", .estimate))
  }
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  m <- mapvalues(model, c("glmnet", "rf", "biscwit"), c("Elastic Net", "Random Forest", "BISCWIT"))
  cap <- sprintf("<strong>Table X</strong><br><em>Tuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants' Best Model of %s Using %s", o, m)
  tab <- tab %>%
    kable(.
          , "html"
          , escape = "F"
          , col.names = cn
          , align = al
          , cap = cap
          ) %>%
    kable_styling(full_width = F)
  save_kable(tab, file = sprintf("%s/05-results/04-tables/03-px-tuning-params/%s_%s.html", local_path, outcome, model))
  return(tab)
}

tuning_param <- param_res %>%
  right_join(best_mods %>% select(-.estimator, -.config)) %>%
  select(-coefs) %>%
  group_by(outcome, model) %>%
  nest() %>%
  ungroup() %>%
  mutate(data = map(data, ~(.) %>% unnest(params) %>% filter(.metric == "accuracy")),
         tab = pmap(list(data, model, outcome), px_tun_par_tab_fun))
```

#### Elastic Net   
Rather than splitting these by outcome and model, I'm going to do a broad discussion across outcomes. From the tables, a few things become clear -- penalties tended to be quite low (near 0) or quite high (near 1). Indeed, of the 10 tested values, only 3 appeared: 0.0000000001, 0.08, and 1.00. For mixture, the most frequent value was 0, but there was was a also more variability than for penalty, with almost all of the 10 possible values being represented. The number of features tended to vary quite widely and does not appear to be a function of stronger penalties or mixture values. The tuning parameters also appear to have little effect on model accuracy.  

##### Procrastination  
```{r}
(tuning_param %>% filter(model == "glmnet" & outcome == "prcrst"))$tab[[1]]
```

##### Loneliness  
```{r}
(tuning_param %>% filter(model == "glmnet" & outcome == "lonely"))$tab[[1]]
```

#### BISCWIT  
##### Procrastination  
The only turning parameter for BISCWIT was the number of items selected through rolling origin validation. As is clear in the table, relative to ENR, BISCWIT tended to select fewer features (e.g., Participant 01 had the full feature set with 22 features for BISCWIT but 49 features for ENR). Divergences in feature numbers are due to ties. As with ENR, the number of features did not appear to be related to the accuracy of the model and the was a wide range in which feature set produced the best model.  

```{r}
(tuning_param %>% filter(model == "biscwit" & outcome == "prcrst"))$tab[[1]]
```

##### Loneliness  
```{r}
(tuning_param %>% filter(model == "biscwit" & outcome == "lonely"))$tab[[1]]
```

#### Random Forest  
Random forest used two tuning parameters, the number of features sampled from the feature set to train the model and the minimum sample size in each group needed for a binary split.  The number of features sampled in each small tree tended to be smaller than the final number of features selected but varied widely across people. The minimum N for a split also varied quite widely. However, 10 was the most frequent number, which logically makes sense given the sample sizes in the present study. Because we used time delay embedding to preserve the "order" of the time series, the final number of features here tended to be larger than other methods (the number of possible features was doubled using an embedding dimension of 1). Each of these appeared to unrelated to accuracy.  
##### Procrastination  

```{r}
(tuning_param %>% filter(model == "rf" & outcome == "prcrst"))$tab[[1]]
```

##### Loneliness  
```{r}
(tuning_param %>% filter(model == "biscwit" & outcome == "lonely"))$tab[[1]]
```


```{r}
param_res %>%
  right_join(best_mods %>% select(-.estimator, -.config)) %>%
  pivot_wider(names_from = ".metric", values_from = ".estimate") %>%
  select(-coefs, -set)  %>%
  unnest(params) %>%
  select(-.config, -merror, -time) %>%
  pivot_longer(cols = c(-(model:group))
               , names_to = "param"
               , values_to = "value"
               , values_drop_na = T) %>%
  group_by(model, outcome, group, param) %>%
  summarize_at(vars(value), lst(mean, sd, min, max)) %>%
  ungroup() %>%
  mutate(mean = sprintf("%.2f (%.2f)", mean, sd),
         range = sprintf("%.2f-%.2f", min, max)) %>%
  select(-sd, -min, -max) %>%
  pivot_wider(names_from = c("outcome", "group")
              , values_from = c("mean", "range")
              , names_glue = "{outcome}_{group}_{.value}") %>%
  mutate(model = factor(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest"))
         , param = factor(param, c("accuracy", "roc_auc", "penalty", "mixture", "nitem", "min_n", "mtry", "nvars"), c("Accuracy", "AUC", "Penalty", "Mixture", "# Items", "Min N Split", "# Predictors Samples", "# Features Selected"))) %>%
  arrange(model, param)
```

## Question 2: Are there individual differences in the idiographic range of prediction across people?  

Next, rather than grouping performance information by the feature sets, we'll group the feature sets by participant, demonstrating the mean, standard deviation, median, and range for each person to answer the range of prediction across people.  

In the manuscript, we include a subset of this as a figure of a sample of 25 participants for each outcome. But below, we'll create tables for each outcome, where each participant is a row to describe their results.  

### The Range of Prediction {.tabset}  

#### Table {.tabset}  

```{r}
px_sum_tab <- function(d, outcome){
  # clean up the outcome names
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  # create the caption
  cap <- sprintf("<strong>Table SX</strong><br><em>Descriptive Statistics of Model Performance for Each Participant for %s", outcome)
  # create the span headers for the table
  h1 <- c(1, rep(2, 6)); names(h1) <- c(" ", rep(c("Accuracy", "AUC"), times = 3))
  h2 <- c(1, rep(4, 3)); names(h2) <- c(" ", "Elastic Net", "BISCWIT", "Random Forest")
  # call the kable table
  tab <- d %>%
    kable(.
          , "html"
          , col.names = c("ID", rep(c("M (SD)", "Range"), times = 6))
          , align = c("r", rep("c", 12))
          , caption = cap
          ) %>%
    kable_styling(full_width = F) %>%
    add_header_above(h1) %>%
    add_header_above(h2) 
  save_kable(tab, file = sprintf("%s/05-results/04-tables/04-participant-sum/%s.html", local_path, outcome))
  return(tab)
}

# indexing the preferred column order
ord <- paste(rep(c("glmnet", "biscwit", "rf"), each = 6)
             , rep(c("accuracy", "roc_auc"), each = 3, times = 3)
             , rep(c("mean", "median", "range"), times = 6)
             , sep = "_")

px_tabs <- sum_res %>% 
  unnest(data) %>%
  group_by(SID, outcome, model, .metric) %>%
  # summaries for each participant, outcome, model, and metric combinations
  summarize_at(vars(.estimate), lst(mean, median, sd, min, max), na.rm = T) %>%
  ungroup() %>%
  mutate(sd = ifelse(sd < .01, "<.01", sprintf("%.2f", sd)),
         mean = sprintf("%.2f (%s)", mean, sd),
         range = sprintf("%.2f-%.2f", min, max),
         median = sprintf("%.2f", median)) %>%
  select(-sd, -min, -max) %>%
  pivot_wider(names_from = c("model", ".metric")
              , values_from = c("mean", "median", "range")
              , names_glue = "{model}_{.metric}_{.value}") %>%
  select(SID, outcome, ord) %>% 
  select(-contains("median")) %>%
  group_by(outcome) %>%
  nest() %>%
  ungroup() %>%
  mutate(tab = map2(data, outcome, px_sum_tab))
```

From the tables below, a few things become clear. First, for both accuracy and AUC, some individuals are very consistent in how accurately we can predict future procrastination or loneliness, regardless of what feature set is used (albeit low or high accuracy). Others, however, show a wide range, with a few participants even showing the full 0 - 1 range (e.g., Participant 135 for procrastination, 154 for loneliness) that is possible for classification accuracy and AUC.  

##### Procrastination  
```{r, results = 'asis'}
px_tabs$tab[[1]]  %>%
    scroll_box(height = "750px")## procrastination
```

##### Loneliness    
```{r, results = 'asis'}
px_tabs$tab[[2]]  %>%
    scroll_box(height = "750px")## loneliness
```

#### Figure (Figure 2) {.tabset}  
Now, we'll create the figure that samples 25 participants ranges. We'll create separate figures for each outcome (Procrastination, Loneliness) and metric (accuracy, AUC), which will be in Supplemental Materials (05-results/05/figures/01-px-sum-dist). Then, we'll create a combined version for accuracy and both outcomes that will become Figure 2 in the manuscript.  

```{r}
px_sum_plot <- function(d, metric, outcome, model){
  m <- if(metric == "accuracy") "Accuracy" else "AUC"
  mod <- mapvalues(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest"), warn_missing = F)
  # set.seed(6)
  d %>%
    # filter(SID %in% sample(SID, 25)) %>%
    mutate(SID = forcats::fct_reorder(SID, .estimate, median)) %>%
      ggplot(aes(x = SID, y = .estimate)) + 
      scale_y_continuous(limits = c(0,1), breaks = seq(0,1,.5)) +
      stat_pointinterval() +
      labs(x = NULL, y = m, title = mod) + 
      coord_flip() + 
      # facet_grid(. ~ , scales = "free", space = "free") + 
      theme_classic() + 
      theme(plot.title = element_text(face = "bold", size = rel(1.2), hjust = .5)
            , axis.text = element_text(face = "bold", color = "black")
            , axis.title = element_text(face = "bold", size = rel(1.1))
            , axis.line = element_blank()
            , panel.background = element_rect(color = "black", size = 1)
            # , plot.margin = margin(1,.1,.1,.1, unit = "cm")
            )
}

combine_px_plots <- function(d, outcome, metric){
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  
  my_theme <- function(...) {
    theme_classic() + 
      theme(plot.title = element_text(face = "bold"))
    }

title_theme <- calc_element("plot.title", my_theme())

ttl <- ggdraw() + 
    draw_label(
        o,
        fontfamily = title_theme$family,
        fontface = title_theme$face,
        size = title_theme$size
    )

  p1 <- d$p[[1]] + labs(y = ""); p2 <- d$p[[2]]; p3 <- d$p[[3]] + labs(y = "")
  p <- cowplot::plot_grid(p1, p2, p3, nrow = 1, axis = "b")
  p <- plot_grid(ttl, p, nrow = 2, rel_heights = c(.05,.95))
  ggsave(p, file = sprintf("%s/05-results/05-figures/01-px-sum-dist/%s_%s.pdf"
                           , local_path, outcome, metric)
         , width = 8
         , height = 5)
  ggsave(p, file = sprintf("%s/05-results/05-figures/01-px-sum-dist/png/%s_%s.png"
                           , local_path, outcome, metric)
         , width = 8
         , height = 5)
  return(p)
}

set.seed(8)
px_plots_sum <- sum_res %>% 
  unnest(data) %>%
  group_by(outcome) %>%
  filter(SID %in% sample(unique(SID), 25)) %>%
  group_by(outcome, .metric, model) %>%
  nest() %>%
  ungroup() %>%
  mutate(model = factor(model, c("glmnet", "biscwit", "rf"))
         , p = pmap(list(data, .metric, outcome, model), px_sum_plot)) %>%
  arrange(outcome, model, .metric) %>%
  group_by(outcome, .metric) %>%
  nest() %>% 
  ungroup() %>%
  mutate(p = pmap(list(data, outcome, .metric), combine_px_plots))

p <- cowplot::plot_grid(px_plots_sum$p[[3]], px_plots_sum$p[[1]]
                   , nrow = 2)
ggsave(p, file = sprintf("%s/05-results/05-figures/fig-2-accuracy.pdf"
                           , local_path)
       , width = 8, height = 10)
```

Figure 2 in the manuscript is the combination of the classification accuracy graphs below. Each of the figures below present the median, 66%, and 95% range of classification accuracy for a random sample of 25 participants, ordered by the median accuracy (AUC is available in the online materials and webapp [“Model Performance Distributions”]). As is clear in the figures, accuracy varies both across people and within them. In other words, although there are between-person differences in the degree of accuracy, there are also within-person differences, depending on which features are used.  

##### Procrastination, Accuracy  
```{r, fig.width=8, fig.height = 5.5, fig.align='center'}
px_plots_sum$p[[3]]
```

##### Procrastination, AUC  
```{r, fig.width=8, fig.height = 5.5, fig.align='center'}
px_plots_sum$p[[4]] 
```

##### Loneliness, Accuracy  
```{r, fig.width=8, fig.height = 5.5, fig.align='center'}
px_plots_sum$p[[1]]
```

##### Loneliness, AUC  
```{r, fig.width=8, fig.height = 5.5, fig.align='center'}
px_plots_sum$p[[2]] 
```

## Question 3: Do Psychological, Situational, or Full Feature Sets Perform Best?  
To answer the question of whether psychological, situational, or full feature sets (with or without time) perform best, we'll pull the performance metric data we've been working with and combine it with information about specific coefficinets.  

### Psychological Features, Situations, or Time?  
#### Table  
```{r}
ord <- paste(rep(c("lonely", "prcrst"), each = 6)
             , rep(c("glmnet", "biscwit", "rf"), each = 2, times = 2)
             , rep(c("n", "perc"), times = 6)
             , sep = "_")

fps_tab <- best_mods %>%
  group_by(model, outcome, group, time, .metric) %>%
  tally() %>%
  group_by(model, outcome, .metric) %>%
  mutate(perc = n/(sum(n, na.rm = T))*100
         , perc = sprintf("%.1f%%",perc)) %>%
  ungroup() %>%
  pivot_wider(names_from = c("outcome", "model")
              , values_from = c("n", "perc")
              , names_glue = "{outcome}_{model}_{.value}"
              , names_sort = T) %>%
  mutate_all(~ifelse(is.na(.), "0", .)) %>%
  arrange(.metric, group, time) %>%
  mutate(group = factor(str_to_title(group))
         , time = factor(time, c("no time", "time"), c("No", "Yes"))) %>%
  select(group, time, ord) %>%
  kable(.
        , "html"
        , escape = F
        , col.names = c("Set", "Time", rep(c("#", "%"), times = 6))
        , align = c("r", "r", rep("c", 12))
        , cap = "<strong>Table 1</strong><br><em>Frequencies of the Full, Psychological, and Situation Feature Sets with or Without Time Being the Best Model for a Participant</em>"
        ) %>%
  kable_styling(full_width = F) %>%
  collapse_rows(1) %>%
  kableExtra::group_rows("Accuracy", 1, 6) %>%
  kableExtra::group_rows("AUC", 7, 12) %>%
  add_header_above(c(" " = 2, "Elastic Net" = 2, "BISCWIT" = 2, "Random Forest" = 2, 
                     "Elastic Net" = 2, "BISCWIT" = 2, "Random Forest" = 2)) %>%
  add_header_above(c(" " = 2, "Loneliness" = 6, "Procrastination" = 6)) 
save_kable(fps_tab, file = sprintf("%s/05-results/04-tables/02-feature-perf-tab.html", local_path))
fps_tab
```

Table 1 presents the number of and percentage of participants whose best model was for each feature set. As is clear, feature sets without time performed better than those with time. Second, relative to AUC, using accuracy as the selection metric was more likely to indicate that the full feature set performed best. Third, with some slight differences, relative proportions were similar across the three methods. Finally, for accuracy but not AUC, only RF indicated that situation feature models performed better than psychological feature models. We next examined the breakdown of selected features for each participant.  

#### Figure (Figure 3)  

Next, to demonstrate the relative performance of feature sets and coefficients within those feature sets, we'll create a series of sequence plots that show the proportion of features from each category for each participant. This will become Figure 3.  

```{r, fig.width=8, fig.height=8, fig.cap="Figure 3. Sequence plots of the percentage of features from the Psychological, Situational, and Time Features Sets for each participant for each outcome."}
seq_plot_fun <- function(d, outcome, model){
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name)
  m <- mapvalues(model, c("glmnet", "biscwit", "rf")
                 , c("Elastic Net", "BISCWIT", "Random Forest")
                 , warn_missing = F)
  ord <- (d %>% 
      select(-n) %>%
      pivot_wider(names_from = "category", values_from = "perc") %>%
      arrange(desc(psychological)))$SID
  p <- d %>%
    mutate(SID = factor(SID, levels = ord),
           category = factor(category,rev(unique(category))
                             , str_to_title(rev(unique(category))))) %>%
    ggplot(aes(x = SID
               , y = perc
               , rev = T)
           ) + 
    scale_fill_manual(values = c("lightgoldenrod1", "seagreen3", "deepskyblue4")) +
    geom_bar(aes(fill = category)
             , stat = "identity"
             ) +
    labs(x = "Participant ID"
         , y = "Percentage"
         , fill = "Feature Category"
         , title = m) + 
    coord_flip() +
    theme_classic() + 
    theme(legend.position = "bottom"
          , axis.text.y = element_blank()
          , axis.ticks.y = element_blank()
          , axis.text.x = element_text(face = "bold", size = rel(1.2), color = "black")
          , axis.title = element_text(face = "bold", size = rel(1.2))
          , legend.text = element_text(face = "bold")
          , legend.title = element_text(face = "bold")
          , plot.title = element_text(face = "bold", hjust = .5)
          )
  return(p)
}
## first get counts and percentages and create each plot for each outcome and model  
seq_plot <- param_res %>% 
  right_join(best_mods %>% filter(.metric == "accuracy") %>% select(model:time)) %>%
  select(-params) %>%
  filter(map_lgl(coefs, is.null) == F) %>%
  mutate(coefs = map(coefs, ~(.) %>% data.frame() %>% rownames_to_column("Variable") %>% setNames(c("Variable", "coef")))) %>%
  unnest(coefs) %>%
  mutate(Variable = str_remove_all(Variable, "_X1"),
         Variable = str_remove_all(Variable, "_1"),
         Variable = str_remove_all(Variable, "_2"),
         Variable = str_replace_all(Variable, "[.]", "_")) %>%
  filter(coef != 0) %>%
  left_join(ftrs %>% select(category = group, Variable = old_name, new_name)) %>%
  filter(!is.na(category)) %>%
  group_by(SID, outcome, model, category) %>%
  tally() %>%
  group_by(SID, outcome, model) %>%
  mutate(perc = n/sum(n)*100) %>%
  group_by(outcome, model) %>% 
  nest() %>%
  ungroup() %>%
  mutate(p = pmap(list(data, outcome, model), seq_plot_fun))

# seq_plot <- seq_plot %>% 
#   unnest(data) %>%
#   group_by(outcome, SID, category) %>%
#   summarize(perc = mean(perc, na.rm = T)) %>%
#   ungroup() %>%
#   mutate(model = "combined") %>%
#   group_by(outcome, model) %>%
#   nest() %>%
#   ungroup() %>%
#   full_join(seq_plot) %>%
#   mutate(p = pmap(list(data, outcome), seq_plot_fun))
my_theme <- function(...) {
  theme_classic() + 
    theme(plot.title = element_text(face = "bold"))
}

title_theme <- calc_element("plot.title", my_theme())

legend <- get_legend(seq_plot$p[[1]])
seq_plot <- seq_plot %>% mutate(p = map(p, ~(.) + theme(legend.position = "none")))
p1 <- plot_grid(
  (seq_plot %>% filter(outcome == "prcrst" & model == "glmnet"))$p[[1]] + 
    labs(y = NULL)
  , (seq_plot %>% filter(outcome == "prcrst" & model == "biscwit"))$p[[1]]+ 
    labs(x = NULL, y = NULL)
  , (seq_plot %>% filter(outcome == "prcrst" & model == "rf"))$p[[1]]+ 
    labs(x = NULL, y = NULL)
  , nrow = 1
  , axis = "b"
  , align = "hv"
)
ttl <- ggdraw() + draw_label("Procrastination", fontface = title_theme$face)  
p1 <- plot_grid(ttl, p1, nrow = 2, rel_heights = c(.05, .95))

p2 <- plot_grid(
  (seq_plot %>% filter(outcome == "lonely" & model == "glmnet"))$p[[1]] + 
    labs(y = NULL, title = NULL)
  , (seq_plot %>% filter(outcome == "lonely" & model == "biscwit"))$p[[1]]+ 
    labs(x = NULL, title = NULL)
  , (seq_plot %>% filter(outcome == "lonely" & model == "rf"))$p[[1]]+ 
    labs(x = NULL, y = NULL, title = NULL)
  , nrow = 1
  , axis = "b"
  , align = "hv"
)
ttl <- ggdraw() + draw_label("Loneliness", fontface = title_theme$face)  
p2 <- plot_grid(ttl, p2, nrow = 2, rel_heights = c(.05, .95))

p <- plot_grid(p1, p2, nrow = 2, rel_heights = c(.6, .4))
p <- plot_grid(p, legend, nrow = 2, rel_heights = c(.95, .05)); p
ggsave(p
       , file = sprintf("%s/05-results/05-figures/fig-3-seq-plot.pdf", local_path)
       , width = 8, height = 8)
ggsave(p
       , file = sprintf("%s/05-results/05-figures/fig-3-seq-plot.png", local_path)
       , width = 8, height = 8)
```

As is clear in Figure 3, which shows proportions of features for all participants’ best models for each method, there were individual differences in the proportion of psychological, situational, and time features. Some participants’ best models included exclusively psychological or situational features, with most showing a varying mixture of both. In addition, as should not be surprising given that Table 1 indicated that random forest was more likely to select the situation feature set as a participant's best model, Figure 3 demonstrates the impact this has on the relative proportion of each type of feature for each outcome.  

## Question 4: Which features are most associated with Procrastination and Loneliness?  
### Feature Frequency: Psychological Features, Situations, or Time?  
To better understand which features were driving differences in which feature set produced the best model for each person, we next examined the variable importance metrics for each participant’s best models. To do so, we extracted the top five features and calculated the proportion of the sample that had each feature in their top five. Then, we created a figure that visually depicts those proportional frequencies of each feature for each outcome and model.     

#### Figure (Figure 4)  

```{r}
var_freq <- param_res %>% 
  right_join(best_mods %>% 
               select(-.estimator, -.config) %>% 
               filter(.metric == "accuracy")
             ) %>%
  filter(map_lgl(coefs, is.null) == F) %>%
  mutate(coefs = map(coefs, ~(.) %>% data.frame() %>% rownames_to_column("Variable") %>% setNames(c("Variable", "coef")))) %>%
  select(-params) %>% 
  unnest(coefs) %>%
  mutate(Variable = str_remove_all(Variable, "_X1"),
         Variable = str_remove_all(Variable, "_1"),
         Variable = str_remove_all(Variable, "_2"),
         Variable = str_replace_all(Variable, "[.]", "_")) %>%
  filter(coef != 0) %>%
  # select(-.) %>%
  group_by(model, SID, outcome) %>% 
  arrange(desc(abs(coef))) %>%
  slice_max(abs(coef), n = 5) %>%
  group_by(model, outcome) %>%
  mutate(N = length(unique(SID))) %>%
  group_by(model, outcome, Variable, N) %>%
  tally() %>%
  ungroup() %>%
  mutate(n = n/N*100) %>%
  left_join(ftrs %>% 
              select(group, Variable = old_name, new_name) %>%
              mutate(group = str_to_title(group)) %>%
              group_by(group) %>%
              mutate(ni = 1:n(), 
                     ni = ifelse(ni < 10, paste0("0", ni), ni),
                     Variable2 = paste0(substr(group, 1, 1), ni)
              ) %>%
              ungroup()
            ) %>%
  filter(!is.na(group)) %>%
  distinct()
```

```{r}
p1 <- var_freq %>%
  filter(outcome == "prcrst") %>%
  mutate(model2 = as.numeric(mapvalues(model, c("glmnet", "biscwit", "rf"), seq(5,1,-2))),
         # model2 = ifelse(outcome == "prcrst", model2 + 1, model2),
         model = factor(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest")),
         new_name = factor(new_name, ftrs$new_name),
         outcome = factor(outcome, outcomes$trait, outcomes$long_name)) %>%
  arrange(model) %>%
  ggplot(aes(x = Variable2
             , y = model2
             , group = factor(Variable2)
             , fill = model
             # , color = model
             # , shape = group
             , rev=F
             )) +
  geom_line(size = .2) + #keep this here, otherwise there is an error
  xlab("") +
  ylab("") +
  # Generate the grid lines
  geom_hline(yintercept = 1:7
             , colour = "grey80"
             , size = .2) +
  geom_vline(xintercept = 1:67
             , colour = "grey80"
             , size = .2) +
  # Points and lines
  geom_line(colour="grey80", size = .2) +
  geom_point(aes(size = n, alpha = n)
             , color = "black"
             , shape = 21) +
  # Fill the middle space with a white blank rectangle
  geom_rect(xmin=-Inf
            ,xmax=Inf
            ,ymin=-Inf
            ,ymax=0
            ,fill="white"
            , color=NA) +
  scale_y_continuous(limits=c(-5,5.5)
                     , expand=c(0,0)
                     , breaks=seq(1,5,2)
                     , labels = NULL) +
  scale_size_continuous(range = c(.5,8)
                        # , limits = c(0, 50) # 10 features
                        # , breaks = c(5, 15, 25, 35, 45)) + # 10 features
                        , limits = c(0, 28) # 5 features
                        , breaks = c(5, 10, 15, 20, 25)) + # 5 features
                        # , limits = c(0, 26.5) # 3 features
                        # , breaks = c(5, 10, 15, 20, 25)) + # 3 features
  scale_alpha_continuous(range = c(.3, 1)
                        # , limits = c(0, 50) # 10 features
                        # , breaks = c(5, 15, 25, 35, 45)) + # 10 features
                        , limits = c(0, 28) # 5 features
                        , breaks = c(5, 10, 15, 20, 25)) + # 5 features
                        # , limits = c(0, 26.5) # 3 features
                        # , breaks = c(5, 10, 15, 20, 25)) + # 3 features
  scale_fill_manual(
        values = c("deepskyblue4", "seagreen3", "lightgoldenrod1")
        , drop = F
        ) +
  # Polar coordinates
  coord_polar() +
  # facet_wrap(~outcome, nrow = 2) + 
  # The angle for the symptoms and remove the default grid lines
  theme_classic()+
  theme(axis.text.x = element_text(angle = 360/(2*pi)*rev( pi/2 + seq(pi/67, 2*pi-pi/67, len=67)) + c(rep(0,floor(67/2)), rep(180,ceiling(67/2))), size = rel(1.1), face = "bold")
        , panel.border = element_blank()
        , axis.line = element_blank()
        , axis.ticks = element_blank()
        , panel.grid = element_blank()
        , panel.background = element_blank()
        , legend.position="bottom"
        # , legend.position = "none"
        # legend.direction = "vertical",
        , plot.margin = margin(t = .5, r = 0, l = 0, b = 0, unit = "cm")
        , plot.title = element_text(face = "bold", hjust = .5)
        , strip.background = element_blank()
        , strip.text = element_text(face = "bold", size = rel(1.2))
        ) +
    labs(size = "% Participants"
         # , fill = "Model"
         , alpha = "% Participants"
         , title = "Procrastination") +
    guides(size = guide_legend(title.position="top", title.hjust = 0.5)
           , fill = "none"#guide_legend(title.position="top", title.hjust = 0.5)
           , alpha = guide_legend(title.position="top", title.hjust = 0.5)
           , shape = guide_legend(title.position="top", title.hjust = 0.5))
legend <- cowplot::get_legend(p1)
p1 <- p1 + theme(legend.position = "none")
p1 <- plot_grid(p1, legend, nrow = 2, rel_heights = c(.9, .1))

mx <- max((var_freq %>%filter(outcome == "lonely"))$n)
p2 <- var_freq %>%
  filter(outcome == "lonely") %>%
  mutate(model2 = as.numeric(mapvalues(model, c("glmnet", "biscwit", "rf"), seq(5,1,-2))),
         # model2 = ifelse(outcome == "prcrst", model2 + 1, model2),
         model = factor(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest")),
         new_name = factor(new_name, ftrs$new_name),
         outcome = factor(outcome, outcomes$trait, outcomes$long_name)) %>%
  arrange(model) %>%
  ggplot(aes(x = Variable2
             , y = model2
             , group = factor(Variable2)
             , fill = model
             # , color = model
             # , shape = group
             , rev=F
             )) +
  geom_line(size = .2) + #keep this here, otherwise there is an error
  xlab("") +
  ylab("") +
  # Generate the grid lines
  geom_hline(yintercept = 1:7
             , colour = "grey80"
             , size = .2) +
  geom_vline(xintercept = 1:67
             , colour = "grey80"
             , size = .2) +
  # Points and lines
  geom_line(colour="grey80", size = .2) +
  geom_point(aes(size = n, alpha = n)
             , color = "black"
             , shape = 21) +
  # Fill the middle space with a white blank rectangle
  geom_rect(xmin=-Inf
            ,xmax=Inf
            ,ymin=-Inf
            ,ymax=0
            ,fill="white"
            , color=NA) +
  scale_y_continuous(limits=c(-8,5.5)
                     , expand=c(0,0)
                     , breaks=seq(1,5,2)
                     , labels = NULL) +
  scale_size_continuous(range = c(.5,8)
                        # , limits = c(0, 50) # 10 features
                        # , breaks = c(5, 15, 25, 35, 45)) + # 10 features
                        , limits = c(0, 35) # 5 features
                        , breaks = c(5, 12.5, 20, 27.5, 35) # 5 features
                        , labels = c("5", "12.5", "20", "27.5", "35")) + 
                        # , limits = c(0, 26.5) # 3 features
                        # , breaks = c(5, 10, 15, 20, 25)) + # 3 features
  scale_alpha_continuous(range = c(.3, 1)
                        # , limits = c(0, 50) # 10 features
                        # , breaks = c(5, 15, 25, 35, 45)) + # 10 features
                        , limits = c(0, 35) # 5 features
                        , breaks = c(5, 12.5, 20, 27.5, 35) # 5 features
                        , labels = c("5", "12.5", "20", "27.5", "35")) + # 5 features
                        # , limits = c(0, 26.5) # 3 features
                        # , breaks = c(5, 10, 15, 20, 25)) + # 3 features
  scale_fill_manual(
        values = c("deepskyblue4", "seagreen3", "lightgoldenrod1")
        , drop = F
        ) +
  # Polar coordinates
  coord_polar() +
  # facet_wrap(~outcome, nrow = 2) + 
  # The angle for the symptoms and remove the default grid lines
  theme_classic()+
  theme(axis.text.x = element_text(angle = 360/(2*pi)*rev( pi/2 + seq(pi/67, 2*pi-pi/67, len=67)) + c(rep(0,floor(67/2)), rep(180,ceiling(67/2))), size = rel(1.1), face = "bold")
        , panel.border = element_blank()
        , axis.line = element_blank()
        , axis.ticks = element_blank()
        , panel.grid = element_blank()
        , panel.background = element_blank()
        , legend.position="bottom"
        # legend.direction = "vertical",
        , plot.margin = margin(t = .5, r = 0, l = 0, b = 0, unit = "cm")
        , plot.title = element_text(face = "bold", hjust = .5)
        , strip.background = element_blank()
        , strip.text = element_text(face = "bold", size = rel(1.2))
        ) +
    labs(size = "% Participants"
         , fill = "Model"
         , alpha = "% Participants"
         , title = "Loneliness") +
    # guides(size = "none"
    #        , fill = guide_legend(title.position="top", title.hjust = .5, label.hjust = 0)
    #        , alpha = "none"
    #        , shape = "none")
    guides(size = guide_legend(title.position="top", title.hjust = .5, order = 1, label.hjust = 0.1)
             , fill = guide_legend(title.position="top", title.hjust = .5, label.hjust = 0)
             , alpha = guide_legend(title.position="top", title.hjust = 0.5, order = 1, label.hjust = 0.1)
             , shape = guide_legend(title.position="top", title.hjust = 0.5, order = 1, label.hjust = 0.1))

legend <- cowplot::get_legend(p2)
p2 <- p2 + theme(legend.position = "none")
  # guides(size = guide_legend(title.position="top", title.hjust = 0.5, order = 1)
  #          , fill = "none"
  #          , alpha = guide_legend(title.position="top", title.hjust = 0.5, order = 1)
  #          , shape = guide_legend(title.position="top", title.hjust = 0.5, order = 1))
  
  p3 <- var_freq %>% 
    select(new_name, Variable2) %>%
    distinct() %>%
    mutate(new_name = factor(new_name, ftrs$new_name),
           names = paste0(Variable2, ": ", new_name)) %>%
    arrange(Variable2) %>%
    mutate(names = factor(names, .$names)) %>%
    ggplot(aes(x = 1, y = 1:66)) +
    geom_text(aes(label = rev(names)), hjust = 0, size = 3) +
    scale_x_continuous(limits = c(.9999, 1.1))+ 
    theme_classic() +
    theme(axis.line = element_blank()
          , axis.text = element_blank()
          , axis.ticks = element_blank()
          , axis.title = element_blank())
  # p3 <- plot_grid(p3, legend, nrow = 2, rel_heights = c(.95, .05)) + 
  #   theme(plot.margin = margin(.1,.5,.5,-1, unit = "cm"))

p <- plot_grid(p1, p2, nrow = 2, rel_heights = c(.53, .47))
p <- plot_grid(p, p3, nrow = 1, rel_widths = c(.65, .35))
p <- plot_grid(p, legend, nrow = 2, rel_heights = c(.95, .05)); p
ggsave(p, file = sprintf("%s/05-results/05-figures/fig-4-combined_top5.pdf", local_path)
       , height = 12 , width = 9)
ggsave(p, file = sprintf("%s/05-results/05-figures/fig-4-combined_top5.png", local_path)
       , height = 12 , width = 9)
```

The resulting Figure 4 has several takeaways. First, across models, timing features were less frequent, with the exception linear, quadratic, and cubic trends (T12-T14) across the ESM period. Second, for ENR and BISCWIT, psychological features were slightly more frequent than situation features. Third, one consequence of the higher frequency of situation feature RF models being selected than for the other two models, top five situation features were both more frequent as well as more variable (more different sized circles) for the RF models than for ENR or BISCWIT (more similarly sized circles). Finally, and perhaps most crucially, this figure makes clear that person and situation characteristics were both key in predicting each outcome, with neither “dominating” the feature space.  

It's noteworthy that this figure depicts relative frequencies of each feature but says nothing about whether certain features were more or less likely to co-occur for each person. This is a question we will return to in Question 5.  

## Question 5: Do people vary in the which features are most important?  

Next we want to address not just general frequencies of important coefficients but also patterns of coefficients at the participant level. To do this, we'll (1) make tables of all coefficients for each participant's best model for all outcomes and models, (2) make a figure that displays this graphically, and (3) examine whether there are patterns of coefficients across people.  

### Participant Coefficients  
#### Table {.tabset}  
First, let's create tables for each participant and outcome combination of the coefficients from their models. As we've previously selected the feature set with the best performance, features from other sets will automatically be set to 0. In addition, as each of the models we used have feature selection procedures, features not chosen by the model will also be 0.  

```{r}
px_coef_tab_fun <- function(d, SID, outcome){
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  mchar <- d %>% select(model, group, accuracy) %>% distinct() %>%
    mutate(model = mapvalues(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest")),
           tmp = sprintf("%s: best model was %s with accuracy %.2f", model, group, accuracy))
  note <- paste(mchar$tmp, collapse = "; "); note <- paste0(note, ".")
  d2 <- d %>% 
    select(-group, -accuracy) %>%
    pivot_wider(names_from = "model"
                , values_from = "coef"
                , values_fn = mean) %>%
    mutate_at(vars(-Variable), ~ifelse(abs(.) > .01, sprintf("%.2f", .), ifelse(. == 0, "0", ifelse(. > -.01 & . < 0, "> -.01", "< .01")))) %>%
    full_join(ftrs %>% select(group, Variable = old_name, new_name)) %>%
    filter(!is.na(group)) %>%
    select(-Variable) %>%
    mutate(new_name = factor(new_name, ftrs$new_name)
           , group = str_to_title(group)) %>%
    mutate_at(vars(glmnet, biscwit, rf), ~ifelse(is.na(.), 0, .)) %>%
    arrange(new_name)
  
  rs <- d2 %>% group_by(group) %>% tally() %>% 
    mutate(end = cumsum(n), start = lag(end) + 1, start = ifelse(is.na(start), 1, start))
  
  tab <- d2 %>%
    select(new_name, glmnet, biscwit, rf) %>%
    kable(.
          , "html"
          , escape = F
          , col.names = c("Variable", "Elastic Net", "BISCWIT", "Random Forest")
          , align = c("r", rep("c", 3))
          , cap = sprintf("%s Model Coefficients for Participant %s", o, SID)) %>%
    kable_styling(full_width = F) %>%
    add_footnote(note, label = NULL)
  for (i in 1:nrow(rs)){
    tab <- tab %>% kableExtra::group_rows(rs$group[i], rs$start[i], rs$end[i])
  }
  save_kable(tab, file = sprintf("%s/05-results/04-tables/05-participant-coef/%s_%s.html"
                                 , local_path, SID, outcome))
  return(tab)
}

px_coef <- param_res %>% 
  right_join(best_mods %>% 
               select(-.estimator, -.config) %>% 
               filter(.metric == "accuracy")
             ) %>%
  filter(map_lgl(coefs, is.null) == F) %>%
  mutate(coefs = map(coefs, ~(.) %>% data.frame() %>% rownames_to_column("Variable") %>% setNames(c("Variable", "coef")))) %>%
  select(-params) %>% 
  unnest(coefs) %>%
  mutate(Variable = str_remove_all(Variable, "_X1"),
         Variable = str_remove_all(Variable, "_1"),
         Variable = str_remove_all(Variable, "_2"),
         Variable = str_replace_all(Variable, "[.]", "_"),
         group = sprintf("%s, %s", str_to_title(group), str_to_title(time))) %>%
  select(-set, -.metric, -time) %>%
  rename(accuracy = .estimate) %>%
  group_by(SID, outcome) %>%
  nest() %>%
  ungroup() %>%
  mutate(tab = pmap(list(data, SID, outcome), possibly(px_coef_tab_fun, NA_real_))); px_coef
```

All of these tables will be contained with the R shiny web app for readers to peruse at their leisure. An in detail description of each is a little beyond the goal and scope of the present study. However, the three sample participants from the manuscript are shown below.  

Notably (and by design), each of these participants differed in the feature set. Participant 169's best model had the psychological feature set; Participant 43 had the situation feature set; and Participant 160 had the full feature set (psychological + situations).  

A few general notes on all these tables. ENR shows log odds coefficient weights, BISCWIT shows zero-order correlation weights, and Random Forest shows permutation-based variable importance metrics. Thus, these are not directly comparable, and observations like that the size of the coefficients tend to be smaller for biscwit are better thought of as relative. The direction magnitude relative to other coefficients / correlations / variable importance metrics tend to be similar across models relative to other variables in the same model. But due to differences in the estimation procedures of these models, they are not (and should not be expected to be) the same across models.  

##### Participant 169  
```{r}
(px_coef %>% filter(SID == "169" & outcome == "prcrst"))$tab[[1]] %>%
    scroll_box(height = "750px")
```

##### Participant 43  
```{r}
(px_coef %>% filter(SID == "43" & outcome == "lonely"))$tab[[1]] %>%
    scroll_box(height = "750px")
```

##### Participant 160  
```{r}
(px_coef %>% filter(SID == "160" & outcome == "prcrst"))$tab[[1]] %>%
    scroll_box(height = "750px")
```

#### Figure {.tabset}  
Now we'll create some figures that display the same information but make relative comparisons within a model easier.  

```{r}
coef_plot_fun <- function(d, outcome, gr, SID, model){
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  mod <- mapvalues(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest"), warn_missing = F)
  ttl <- sprintf("Best %s Model (%s) Predicting \n%s for Participant %s", mod, gr, o, SID)
  d <- d %>% mutate(neg = ifelse(sign(coef) == -1, "(-)", NA), coef = abs(coef))
  # Set a number of 'empty bar' to add at the end of each group
  empty_bar <- 4
  to_add <- data.frame(matrix(NA, empty_bar*nlevels(factor(d$category)), ncol(d)) )
  colnames(to_add) <- colnames(d)
  to_add$category <- rep(levels(factor(d$category)), each=empty_bar)
  d <- rbind(d, to_add)
  d <- d %>% arrange(category, desc(coef))
  d$id <- seq(1, nrow(d))
  
  
  breaks <- round(seq(0, max(d$coef, na.rm = T), length.out = 5),2)
  rng <- c(-1*(breaks[5]-.01), breaks[5]+.01)
   
  # Get the name and the y position of each label
  label_data <- d
  number_of_bar <- nrow(label_data)
  angle <- 90 - 360 * (label_data$id-0.5) /number_of_bar     # I substract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0)
  label_data$hjust <- ifelse( angle < -90, 1, 0)
  label_data$angle <- ifelse(angle < -90, angle+180, angle)
  label_data <- label_data %>% 
    mutate(y = ifelse(is.na(coef) | coef < 0, 0, coef + rng[2]/20)
           , lab = ifelse(!is.na(coef) & coef > 0, str_wrap(`short name`, 20), `short name`))
  rng <- c(round(-1*max(label_data$y),2)-.01, round(max(label_data$y),2)+.01)
  
  # prepare a data frame for base lines
  base_data <- d %>%
    group_by(category) %>%
    summarize(start=min(id), end=max(id) - empty_bar) %>%
    rowwise() %>%
    mutate(title=mean(c(start, end)))
 
  
  breaks <- breaks[1:4]
  # rng <- c(-1*breaks[4], breaks[4])
  # prepare a data frame for grid (scales)
  grid_data <- base_data
  grid_data$end <- grid_data$end[ c( nrow(grid_data), 1:nrow(grid_data)-1)] + 1
  grid_data$start <- grid_data$start - 1
  grid_data <- grid_data[-1,]
  # grid_data <- grid_data %>% crossing(breaks)
  
  p <- d %>%
    ggplot(aes(
      x = as.factor(id)
      , y = coef
      , fill = category
    )) +
      geom_bar(aes(na.rm = F), stat="identity", alpha=0.5) + 
      scale_x_discrete(drop=FALSE) +
      scale_fill_manual(
        values = c("deepskyblue4", "seagreen3", "lightgoldenrod1")
        , drop = F
        ) +
  
    # Add a val=100/75/50/25 lines. I do it at the beginning to make sur barplots are OVER it.
      geom_segment(data = grid_data
                   , aes(x = end, y = breaks[1], xend = start, yend = breaks[1])
                   , colour = "grey"
                   , alpha=1
                   , size=0.3
                   , inherit.aes = FALSE ) + 
    geom_segment(data = grid_data
                   , aes(x = end, y = breaks[2], xend = start, yend = breaks[2])
                   , colour = "grey"
                   , alpha=1
                   , size=0.3
                   , inherit.aes = FALSE ) + 
    geom_segment(data = grid_data
                   , aes(x = end, y = breaks[3], xend = start, yend = breaks[3])
                   , colour = "grey"
                   , alpha=1
                   , size=0.3
                   , inherit.aes = FALSE ) + 
    geom_segment(data = grid_data
                   , aes(x = end, y = breaks[4], xend = start, yend = breaks[4])
                   , colour = "grey"
                   , alpha=1
                   , size=0.3
                   , inherit.aes = FALSE ) + 
      #  # Add text showing the value of each 100/75/50/25 lines
      annotate("text"
               , x = rep(max(d$id),4)
               , y = breaks
               , label = paste0(breaks, "-")
               , color="grey"
               , size=3
               , angle=0
               , fontface="bold"
               , hjust=1) +
      ylim(rng[1], rng[2]) + 
      theme_minimal() +
      theme(
        # legend.position = "none",
        legend.position = "bottom",
        axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.title = element_text(hjust = .5, size = rel(1))
        # plot.margin = unit(rep(-1,4), "cm") 
      ) +
      coord_polar() +
      labs(fill = "Feature Category", title = ttl) + 
      guides(fill = guide_legend(title.position="top", title.hjust = .5, label.hjust = 0)) + 
      geom_text(data = label_data
                , aes(x = id
                      , y = y#coef
                      , label = lab
                      , hjust = hjust)
                , color="black"
                , fontface="bold"
                , lineheight = .6
                , alpha=0.6
                , size=2.5
                , angle= label_data$angle
                , inherit.aes = FALSE) + 
    geom_text(data = label_data
                , aes(x = id
                      , y = coef - rng[2]/20
                      , label = neg)
                , color="black"
                , hjust = .5
                , fontface="bold"
                , lineheight = .6
                , size=2
                , angle= label_data$angle
                , inherit.aes = FALSE) + 
    # Add base line information
    geom_segment(data = base_data
                 , aes(x = start
                       , y = rng[1]/20
                       , xend = end
                       , yend = rng[1]/20)
                 , colour = "black"
                 , alpha=0.8
                 , size=0.6 
                 , inherit.aes = FALSE) 
  ggsave(p, filename = sprintf("%s/05-results/05-figures/02-participant-coef/%s/%s_%s.pdf"
                            , local_path, model, SID, outcome)
         , width = 6, height = 8)
  ggsave(p, filename = sprintf("%s/05-results/05-figures/02-participant-coef/%s/png/%s_%s.png"
                            , local_path, model, SID, outcome)
         , width = 6, height = 8)
  return(p)
}


px_coef_fig <- param_res %>% 
  right_join(best_mods %>% filter(.metric == "accuracy") %>% select(model:time)) %>% 
  select(-params) %>% 
  filter(map_lgl(coefs, is.null) == F) %>% 
  mutate(coefs = map(coefs, ~(.) %>% 
                       data.frame() %>% 
                       rownames_to_column("Variable") %>% 
                       setNames(c("Variable", "coef")))
         ) %>%
    unnest(coefs) %>%
    mutate(Variable = str_remove_all(Variable, "_X1"),
           Variable = str_remove_all(Variable, "_1"),
           Variable = str_remove_all(Variable, "_2"),
           Variable = str_replace_all(Variable, "[.]", "_"),
         group = sprintf("%s, %s", str_to_title(group), str_to_title(time))) %>%
  select(-set, -time) %>%
  group_by(model, SID, outcome, group, Variable) %>%
  summarize(coef = mean(coef)) %>%
  ungroup() %>%
  group_by(model, outcome, SID, group) %>%
  nest() %>%
  ungroup() %>%
  mutate(data = map(data, ~(.) %>% 
                      full_join(ftrs %>% 
                                  select(category = group, Variable = old_name, `short name`)
                                ) %>%
                      mutate(coef = ifelse(coef == 0, NA_real_, coef)
                             , category = factor(str_to_title(category)))),
         p = pmap(list(data, outcome, group, SID, model)
                  , possibly(coef_plot_fun, NA_real_)))
```

As before, we'll show our three example participants. These figures are present in the manuscript as Figures 5-7.  

##### Participant 169 (Figure 5)  
Participant 169’s best model for procrastination used the psychological feature set without time for each of the three methods (accuracy = 0.94; AUC = 0.80). Variable importance (log odds ratios) for the features in their ENR model are shown in the bar graph in Figure 5. Across all three methods, there were some differences selected features, but consensus in the direction and general magnitude of them. Across all three, the top feature was the Openness to Experience facet Creative Imagination, perhaps indicating that this participant tended to procrastinate when they were feeling more creative or imaginative previously. As in clear in Figure 5, they also tended to procrastinate less when they were Intellectually Curious (O) and more when they felt afraid. Thus, it seems like this participant’s procrastination may partially hinge upon competition between intellectual and creative pursuits, as well general fears.  

```{r, fig.width = 6, fig.height = 8, fig.cap="Figure 5. Variable Importance (absolute value of log odds) for Participant 169’s best model predicting Procrastination. Each bar is a different feature. Color indicates feature category (psychological, situational, or time). Height of bars indicates the magnitude of the effect. (-)indicate negative effects (i.e. lower odds)."}
(px_coef_fig %>% filter(outcome == "prcrst" & SID == "169" & model == "glmnet"))$p[[1]]
ggsave(file = sprintf("%s/05-results/05-figures/fig-5-px-169.pdf", local_path)
       , width = 6, height = 8)
```

##### Participant 43 (Figure 6)  
participant 43’s best model for loneliness used the situation feature set (without time; accuracy = 0.91; AUC = 0.83). As in seen in the bar graph of their ENR variable importance in Figure 6, the situation characteristics and features seem to indicate that obligations (e.g., duty, in class), physical health (e.g., sick, sleeping), and social interactions (Sociability, argument) were predictive of future feelings of loneliness. For example, both the situation feature Duty and being in class predicted less loneliness, while feeling sick and getting in an argument predicted more.  

```{r, fig.width = 6, fig.height = 8, fig.cap="Figure 6. Variable Importance (absolute value of log odds) for Participant 43’s best model predicting Loneliness. Each bar is a different feature. Color indicates feature category (psychological, situational, or time). Height of bars indicates the magnitude of the effect. (-) indicates negative effects."}
(px_coef_fig %>% filter(outcome == "lonely" & SID == "43" & model == "glmnet"))$p[[1]]
ggsave(file = sprintf("%s/05-results/05-figures/fig-6-px-43.pdf", local_path)
       , width = 6, height = 8)
```

##### Participant 160 (Figure 7)  
Participant 160’s best models for procrastination utilized the full feature set (i.e. psychological and situational features) without time (see Figure 7; accuracy = 0.89, AUC = 0.94). ENR and BISCWIT agreed on the top three features: Sociability (DIAMONDS; negative), Sleeping (positive), and Depression (Neuroticism; negative) were each associated with future procrastination. Moreover, other related features, like attentiveness and Assertiveness (Extraversion), were also predictive of both outcomes.  

```{r, fig.width = 6, fig.height = 8, fig.cap="Figure 7. Variable Importance (absolute value of log odds) for Participant 160’s best model predicting Procrastination. Each bar is a different feature. Color indicates feature category (psychological, situational, or time). Height of bars indicates the magnitude of the effect. (-) indicates negative effects."}
(px_coef_fig %>% filter(outcome == "prcrst" & SID == "160" & model == "glmnet"))$p[[1]]
ggsave(file = sprintf("%s/05-results/05-figures/fig-7-px-160.pdf", local_path)
       , width = 6, height = 8)
```

### Profile Similarity  
Finally, when we looked at relative proportional frequencies of different features appearing in particiapnts' top fives, it told us nothing about the tendency of features to co-occur. To look at this, we'll opt for a simple visual depiction of the profile of coefficients/correlations/variable importance for each participant's best model for each metric (accuracy, AUC), outcome (procrastination, loneliness), and model (ENR, BISCWIT, RF).  
```{r}
procor_fun <- function(d){
  m <- d %>% 
    select(-SID) %>%
    mutate_all(~ifelse(is.na(.), 0, .)) %>%
    as.matrix(); rownames(m) <- d$SID
  r <- cor(t(m))
  diag(r) <- NA
  rd <- r %>% data.frame() %>% 
    rownames_to_column("SID1") %>%
    pivot_longer(cols = -SID1
                 , names_to = "SID2"
                 , values_to = "r"
                 , values_drop_na = T) %>%
    mutate(SID2 = str_remove_all(SID2, "X"))
  
  r %>%
    mutate(r = fisherz(r)) %>%
    group_by(SID1) %>%
    summarize_at(vars(r), lst(mean, min, max))
}

profile_sim <- param_res %>%
  select(-params) %>%
  right_join(best_mods %>% select(model:.metric)) %>%
  filter(map_lgl(coefs, is.null) == F) %>% 
  mutate(coefs = map(coefs, ~(.) %>% 
                       data.frame() %>% 
                       rownames_to_column("Variable") %>% 
                       setNames(c("Variable", "coef")))
         ) %>%
    unnest(coefs) %>%
    mutate(Variable = str_remove_all(Variable, "_X1"),
           Variable = str_remove_all(Variable, "_1"),
           Variable = str_remove_all(Variable, "_2"),
           Variable = str_replace_all(Variable, "[.]", "_")) %>%
    filter(Variable %in% ftrs$old_name) %>%
  group_by(model, SID, outcome, .metric, Variable) %>%
  summarize(coef = mean(coef)) %>%
  ungroup()

profile_plot_fun <- function(d, model, metric, outcome){
  o <- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F)
  mod <- mapvalues(model, c("glmnet", "biscwit", "rf"), c("Elastic Net", "BISCWIT", "Random Forest"), warn_missing = F)
  m <- mapvalues(metric, c("accuracy", "roc_auc"), c("Accuracy", "AUC"), warn_missing = F)
  ttl <- sprintf("%s Predicting Future %s Using Best %s Models", mod, o, m)
  min <- if(model == "glmnet") 5 else if (model == "biscwit") 1 else max(abs(d$coef), na.rm = T)
  p <- d %>%
  mutate(coef = ifelse(coef == 0, NA, coef)
         , coef = ifelse(coef > min, min, ifelse(coef < -1*min, -1*min, coef))
         , group = str_to_title(group)
         , new_name = factor(new_name, rev(ftrs$new_name))) %>%
  drop_na() %>%
  ggplot(aes(x = SID, y = new_name, color = coef)) + 
    scale_color_gradient2(low = "blue"
                          , mid = "white"
                          , high = "red"
                          # , limits = c(-5,5)
                          ) + 
    geom_point() + 
    labs(x = "Participant ID", y = NULL, color = "Coefficient"
         , title = ttl) + 
    facet_grid(group ~ ., space = "free", scale = "free") + 
    theme_classic() + 
    theme(axis.text.x = element_text(angle = 90, face = "bold")
          , axis.text.y = element_text(face = "bold")
          , legend.position = "bottom"
          , strip.background = element_rect(fill = "black")
          , strip.text = element_text(face = "bold", color = "white", size = rel(1.2)))
  ggsave(p, file = sprintf("%s/05-results/05-figures/03-px-profiles/%s_%s_%s.pdf"
                           , local_path, outcome, model, metric)
         , width = 10, height = 10)
  ggsave(p, file = sprintf("%s/05-results/05-figures/03-px-profiles/png/%s_%s_%s.png"
                           , local_path, outcome, model, metric)
         , width = 10, height = 10)
  return(p)
}
  
profile_sim_plots <- profile_sim %>%
  left_join(ftrs %>% select(group, Variable = old_name, new_name)) %>%
  group_by(model, .metric, outcome) %>%
  nest() %>%
  ungroup() %>%
  mutate(p = pmap(list(data, model, .metric, outcome), profile_plot_fun))
```

For parsimony, I'm just going to display the accuracy plots. You'll find the others in the online materials and webapp.  

#### Elastic Net  
Broadly there are a few takeaways from each of these figures. First, relative to other models (see below), ENR tended to include more features in any given model. However, likely due to the the combination of both soft and hard constraints via L1 and L2 regularization, the magnitudes were relatively low for some individuals than others. Second, only a relatively small number of participants' best models had timing features. Third, even common features varied widely across people in presence, direction, and magnitude without exception. Finally, no two profiles are the same even just in which features were included, let alone in direction and magnitude of the associations.  
##### Procrastination  
```{r}
(profile_sim_plots %>% filter(.metric == "accuracy" & model == "glmnet" & outcome == "prcrst"))$p[[1]]
```

##### Loneliness  
```{r}
(profile_sim_plots %>% filter(.metric == "accuracy" & model == "glmnet" & outcome == "lonely"))$p[[1]]
```


#### BISCWIT  
Broadly there are a few takeaways from each of these figures. First, the relative magnitude of the correlations tended to be stronger in the positive direction than the negative one. Second, only a relatively small number of participants' best models had timing features. Third, even common features varied widely across people in presence, direction, and magnitude without exception. Finally, no two profiles are the same even just in which features were included, let alone in direction and magnitude of the associations.    
##### Procrastination  
```{r}
(profile_sim_plots %>% filter(.metric == "accuracy" & model == "biscwit" & outcome == "prcrst"))$p[[1]]
```

##### Loneliness  
```{r}
(profile_sim_plots %>% filter(.metric == "accuracy" & model == "biscwit" & outcome == "lonely"))$p[[1]]
```

#### Random Forest  
##### Procrastination  
```{r}
(profile_sim_plots %>% filter(.metric == "accuracy" & model == "rf" & outcome == "prcrst"))$p[[1]]
```

##### Loneliness  
```{r}
(profile_sim_plots %>% filter(.metric == "accuracy" & model == "rf" & outcome == "lonely"))$p[[1]]
```

<!--chapter:end:05-summary.Rmd-->

# Extended Discussion  

The current study investigated personalized, idiographic prediction models for two behaviors, feeling lonely and procrastinating. Rather than assuming that antecedents of different outcomes were shared, our idiographic approach built N=1, personalized prediction models. Overall, three main conclusions emerged: First, psychological, situational, and time variables accurately predicted future everyday behaviors. Second, psychological and situational variables were both important, almost equally so, with neither being a predominant antecedent of behavior. Third, individual differences reigned supreme –people differed on how predictable outcomes were, which domains performed best, and which features were most important. These findings indicate the utility of an idiographic approach to psychological assessment relative to standard between-person approaches that are routinely used.  

## On predicting more behaviors more of the time  
We found accurate out-of-sample prediction of procrastination and feelings of loneliness when using a suite of personality and situational factors. While there are between-person individual differences in both loneliness and procrastination, there was also within-person variability in terms of how and when people experienced these behaviors. Typical prediction models within psychology have largely focused on which between-person features predict life outcomes or other aggregated behaviors (e.g., Beck & Jackson, 2021a; Joel et al., 2020; Puterman et al., 2020). Here, in alignment with a growing emphasis on precision medicine approaches to improving physical health, well-being, and productivity, we demonstrate that within-person features are also predictable by psychological and situation features. These dynamic features tend to be less studied, which has resulted in little knowledge about why people vary within-person in these behaviors. Our findings suggest that from a fairly prescribed set of personality, situational, and time features, we can identify when someone is going to procrastinate or feel lonely at a future timepoint – not just if they tend to procrastinate or feel lonely in general.   

Notably, predictions were made assuming individuals have unique antecedents of each behavior. Although this equifinality is often described in theoretical models, it is rarely implemented in statistical models. Instead, statistical models use a circumscribed set of predictors that are assumed to impact people similarly, depending on their rank-order on the predictor (e.g., Borsboom et al., 2003). For example, procrastination is associated with Conscientiousness (Jackson et al., 2009). Typically, this suggests if people are feeling low in Conscientiousness markers (responsibility, organization) they would be more likely to procrastinate. However, we found that markers of Conscientiousness were not important antecedents of procrastinating for everyone, nor were they the most important in general (with 10-15% of the sample having Conscientiousness features as important predictors). People both procrastinate and feel lonely for many different reasons. As a result, prediction models that assume similar associations between predictors and outcomes for everyone may underestimate potential predictive validity.  

In general, we found individual differences in every aspect of the models – in accuracy, in feature sets, and in the importance of specific features. For some people, we could highly accurately predict future behaviors, while for others, we could not. Similarly, people differed in which and the degree to which the domains were important. Together these findings paint a picture of a psychological system that is highly unique to an individual. Although there is a longstanding consensus that behavior is the output of such highly unique dynamic psychological systems that are impacted by situational features (Mischel & Shoda, 1995), these have remained elusive and often ignored in practice. Thus, the present study is an initial demonstrate of the empirical validity of such thinking. These participants demonstrated unique important situational and psychological features predicted future behavior.  

## The person situation debate revisited   

Half a century ago, the seeming limits of behavioral prediction that sparked the Person-Situation Debate and led to research being formulated around the question of whether person or situation features matter more. While most agree that both matter, there are few examples of demonstrating the joint importance of them for the same outcome (c.f., Sherman et al., 2015). We found evidence that person and situation features were both important for most individuals, with only a minority demonstrating that person or situation features alone were most predictive of future procrastination or loneliness. In other words, the Person-Situation Debate was always a false debate. The dynamic relations among person, situation, and behavior and indicate that attempts to understand behavior must incorporate both (Funder, 2006) – at least for most people.  

Not only are person and situation variables important, but they were also more important than time variables. Given that people have natural cycles of behavior that are regimented by time of day and day of week (Mathews, 1988; Larson, 1985), it would be natural to expect that behavior largely varies within and across people as a function of these cycles. For example, people work (behavior) less on the weekends and at night, which is a change their behavior. Similarly, time of day and day of week govern situations people can enter. Why were time variables not that important? It is likely that these time indices were already captured by the more proximal person or situational features. Time is likely important, but works through person and situation variables rather than being a separate factor.  

## Limitations and Conclusion  
This study is not without its limitations. First, relatively low variance in procrastination and loneliness led us to drop a number of participants from analyses. Thus, the participants in the present study are only representative of participants who experienced somewhat frequent loneliness and procrastination. Second, we examined prediction over a two week interval for most participants, so long-term prediction accuracy is unclear. Finally, we demonstrated high accuracy and AUC on average when predicting behavior four hours in the future, making it unclear how such models perform at different time intervals. However, given that processes unfold at different speeds both within- and between-person, model performance likely varies as a function of interval.  

The current study created personalized prediction models to help understand antecedents of future loneliness and procrastination. We found psychological and situational predictors did well in predicting within-person variations in these behaviors. However, in contrast to many years of methodological orthodoxy, the antecedents of these behaviors differed greatly across people. Thus, there is a need for more personalized assessments – not just longer assessments – but assessments that are tailored and important for the individual. Behavior appears to be highly predictable, so our next task is identifying personalized antecedents.  

<!--chapter:end:06-extended-discussion.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

